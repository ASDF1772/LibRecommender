{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "train_set = open(\"ml-1m/ratings.dat\").readlines()\n",
    "train_set = np.random.permutation(train_set)\n",
    "train_threshold = int(0.75 * len(train_set))\n",
    "# train_indices = []\n",
    "# test_indices = []\n",
    "train_user_indices = []\n",
    "train_item_indices = []\n",
    "test_user_indices = []\n",
    "test_item_indices = []\n",
    "train_ratings = []\n",
    "test_ratings = []\n",
    "\n",
    "data = defaultdict(dict)\n",
    "user2id = {}\n",
    "item2id = {}\n",
    "index_user = 0\n",
    "index_item = 0\n",
    "\n",
    "for i, line in enumerate(train_set):\n",
    "    user = line.split(\"::\")[0]\n",
    "    item = line.split(\"::\")[1]\n",
    "    rating = line.split(\"::\")[2]\n",
    "    \n",
    "    try:\n",
    "        user_id = user2id[user]\n",
    "    except KeyError:\n",
    "        user_id = index_user\n",
    "        user2id[user] = index_user\n",
    "        index_user += 1\n",
    "    try:\n",
    "        item_id = item2id[item]\n",
    "    except KeyError:\n",
    "        item_id = index_item\n",
    "        item2id[item] = index_item\n",
    "        index_item += 1\n",
    "        \n",
    "    if i < train_threshold:\n",
    "    #    train_indices.append((user_id, item_id))\n",
    "        train_user_indices.append(user_id)\n",
    "        train_item_indices.append(item_id)\n",
    "        train_ratings.append(int(rating))\n",
    "        data[user_id].update(dict(zip([item_id], [int(rating)])))\n",
    "    else:\n",
    "    #    test_indices.append((user_id, item_id))\n",
    "        test_user_indices.append(user_id)\n",
    "        test_item_indices.append(item_id)\n",
    "        test_ratings.append(int(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750156, 250053)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ratings), len(test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  7500 2500\n",
      "after:  7500 1975\n"
     ]
    }
   ],
   "source": [
    "print(\"before: \", len(train_ratings), len(test_ratings))\n",
    "\n",
    "for u, i, r in zip(test_user_indices, test_item_indices, test_ratings):\n",
    "    if u not in data:\n",
    "        test_user_indices.remove(u)\n",
    "        test_item_indices.remove(i)\n",
    "        test_ratings.remove(r)\n",
    "\n",
    "for u, i, r in zip(test_user_indices, test_item_indices, test_ratings):\n",
    "    if u not in data:\n",
    "        test_user_indices.remove(u)\n",
    "        test_item_indices.remove(i)\n",
    "        test_ratings.remove(r)\n",
    "\n",
    "for u, i, r in zip(test_user_indices, test_item_indices, test_ratings):\n",
    "    if u not in data:\n",
    "        test_user_indices.remove(u)\n",
    "        test_item_indices.remove(i)\n",
    "        test_ratings.remove(r)\n",
    "\n",
    "for u in test_user_indices:\n",
    "    if u not in data:\n",
    "        print(\"left\", u)\n",
    "\n",
    "print(\"after: \", len(train_ratings), len(test_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.array([train_user_indices, train_item_indices]).T\n",
    "test_indices = np.array([test_user_indices, test_item_indices]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \ttrain loss:  1.1099772 \ttest loss:  1.1103163\n",
      "Epoch:  6 \ttrain loss:  1.0758973 \ttest loss:  1.0770359\n",
      "Epoch:  11 \ttrain loss:  1.0183473 \ttest loss:  1.0220661\n",
      "Epoch:  16 \ttrain loss:  0.95823574 \ttest loss:  0.9663194\n",
      "Epoch:  21 \ttrain loss:  0.94376427 \ttest loss:  0.95290095\n",
      "Epoch:  26 \ttrain loss:  0.9421018 \ttest loss:  0.95110947\n",
      "Epoch:  31 \ttrain loss:  0.9393327 \ttest loss:  0.94824326\n",
      "Epoch:  36 \ttrain loss:  0.93477565 \ttest loss:  0.9437178\n",
      "Epoch:  41 \ttrain loss:  0.9305752 \ttest loss:  0.9395445\n",
      "Epoch:  46 \ttrain loss:  0.92739445 \ttest loss:  0.9363242\n",
      "Epoch:  51 \ttrain loss:  0.9249203 \ttest loss:  0.93377566\n",
      "Epoch:  56 \ttrain loss:  0.92283916 \ttest loss:  0.9316009\n",
      "Epoch:  61 \ttrain loss:  0.92104363 \ttest loss:  0.9297234\n",
      "Epoch:  66 \ttrain loss:  0.9194193 \ttest loss:  0.92803425\n",
      "Epoch:  71 \ttrain loss:  0.91791236 \ttest loss:  0.9264875\n",
      "Epoch:  76 \ttrain loss:  0.9165547 \ttest loss:  0.92510307\n",
      "Epoch:  81 \ttrain loss:  0.91538274 \ttest loss:  0.9239051\n",
      "Epoch:  86 \ttrain loss:  0.9144016 \ttest loss:  0.922891\n",
      "Epoch:  91 \ttrain loss:  0.9135817 \ttest loss:  0.9220334\n",
      "Epoch:  96 \ttrain loss:  0.9128833 \ttest loss:  0.9213016\n",
      "Epoch:  101 \ttrain loss:  0.91228324 \ttest loss:  0.9206747\n",
      "Epoch:  106 \ttrain loss:  0.9117667 \ttest loss:  0.92013705\n",
      "Epoch:  111 \ttrain loss:  0.9113303 \ttest loss:  0.91968375\n",
      "Epoch:  116 \ttrain loss:  0.9109715 \ttest loss:  0.91930515\n",
      "Epoch:  121 \ttrain loss:  0.91067195 \ttest loss:  0.91899186\n",
      "Epoch:  126 \ttrain loss:  0.9104167 \ttest loss:  0.9187269\n",
      "Epoch:  131 \ttrain loss:  0.91020215 \ttest loss:  0.91850257\n",
      "Epoch:  136 \ttrain loss:  0.91002065 \ttest loss:  0.9183144\n",
      "Epoch:  141 \ttrain loss:  0.90986884 \ttest loss:  0.91815674\n",
      "Epoch:  146 \ttrain loss:  0.90973943 \ttest loss:  0.9180223\n",
      "Epoch:  151 \ttrain loss:  0.9096286 \ttest loss:  0.91790724\n",
      "Epoch:  156 \ttrain loss:  0.90953463 \ttest loss:  0.91781133\n",
      "Epoch:  161 \ttrain loss:  0.90945405 \ttest loss:  0.9177301\n",
      "Epoch:  166 \ttrain loss:  0.9093859 \ttest loss:  0.9176611\n",
      "Epoch:  171 \ttrain loss:  0.9093272 \ttest loss:  0.91760117\n",
      "Epoch:  176 \ttrain loss:  0.90927744 \ttest loss:  0.91755176\n",
      "Epoch:  181 \ttrain loss:  0.90923434 \ttest loss:  0.91751003\n",
      "Epoch:  186 \ttrain loss:  0.90919805 \ttest loss:  0.9174731\n",
      "Epoch:  191 \ttrain loss:  0.90916723 \ttest loss:  0.91744274\n",
      "Epoch:  196 \ttrain loss:  0.9091397 \ttest loss:  0.9174171\n",
      "Epoch:  201 \ttrain loss:  0.9091165 \ttest loss:  0.91739506\n",
      "Epoch:  206 \ttrain loss:  0.9090965 \ttest loss:  0.9173762\n",
      "Epoch:  211 \ttrain loss:  0.9090798 \ttest loss:  0.9173605\n",
      "Epoch:  216 \ttrain loss:  0.9090658 \ttest loss:  0.9173464\n",
      "Epoch:  221 \ttrain loss:  0.9090536 \ttest loss:  0.9173348\n",
      "Epoch:  226 \ttrain loss:  0.90904325 \ttest loss:  0.91732526\n",
      "Epoch:  231 \ttrain loss:  0.9090342 \ttest loss:  0.91731685\n",
      "Epoch:  236 \ttrain loss:  0.9090265 \ttest loss:  0.9173097\n",
      "Epoch:  241 \ttrain loss:  0.90901965 \ttest loss:  0.9173035\n",
      "Epoch:  246 \ttrain loss:  0.9090137 \ttest loss:  0.9172977\n",
      "Epoch:  251 \ttrain loss:  0.9090086 \ttest loss:  0.9172934\n",
      "Epoch:  256 \ttrain loss:  0.90900433 \ttest loss:  0.91728973\n",
      "Epoch:  261 \ttrain loss:  0.9090009 \ttest loss:  0.91728675\n",
      "Epoch:  266 \ttrain loss:  0.9089975 \ttest loss:  0.91728395\n",
      "Epoch:  271 \ttrain loss:  0.90899473 \ttest loss:  0.91728145\n",
      "Epoch:  276 \ttrain loss:  0.90899235 \ttest loss:  0.9172795\n",
      "Epoch:  281 \ttrain loss:  0.9089904 \ttest loss:  0.91727763\n",
      "Epoch:  286 \ttrain loss:  0.90898865 \ttest loss:  0.91727614\n",
      "Epoch:  291 \ttrain loss:  0.90898705 \ttest loss:  0.917275\n",
      "Epoch:  296 \ttrain loss:  0.90898556 \ttest loss:  0.9172739\n",
      "Epoch:  301 \ttrain loss:  0.90898436 \ttest loss:  0.91727287\n",
      "Epoch:  306 \ttrain loss:  0.90898323 \ttest loss:  0.91727203\n",
      "Epoch:  311 \ttrain loss:  0.90898234 \ttest loss:  0.9172712\n",
      "Epoch:  316 \ttrain loss:  0.9089816 \ttest loss:  0.91727066\n",
      "Epoch:  321 \ttrain loss:  0.9089809 \ttest loss:  0.91727036\n",
      "Epoch:  326 \ttrain loss:  0.90898025 \ttest loss:  0.9172699\n",
      "Epoch:  331 \ttrain loss:  0.90897983 \ttest loss:  0.91726935\n",
      "Epoch:  336 \ttrain loss:  0.90897936 \ttest loss:  0.91726905\n",
      "Epoch:  341 \ttrain loss:  0.9089791 \ttest loss:  0.9172688\n",
      "Epoch:  346 \ttrain loss:  0.9089789 \ttest loss:  0.9172686\n",
      "Epoch:  351 \ttrain loss:  0.90897864 \ttest loss:  0.91726834\n",
      "Epoch:  356 \ttrain loss:  0.9089785 \ttest loss:  0.91726816\n",
      "Epoch:  361 \ttrain loss:  0.90897816 \ttest loss:  0.9172678\n",
      "Epoch:  366 \ttrain loss:  0.908978 \ttest loss:  0.91726774\n",
      "Epoch:  371 \ttrain loss:  0.90897787 \ttest loss:  0.9172676\n",
      "Epoch:  376 \ttrain loss:  0.9089778 \ttest loss:  0.91726774\n",
      "Epoch:  381 \ttrain loss:  0.9089777 \ttest loss:  0.9172675\n",
      "Epoch:  386 \ttrain loss:  0.9089774 \ttest loss:  0.91726744\n",
      "Epoch:  391 \ttrain loss:  0.90897727 \ttest loss:  0.91726744\n",
      "Epoch:  396 \ttrain loss:  0.9089773 \ttest loss:  0.91726744\n",
      "Epoch:  401 \ttrain loss:  0.90897727 \ttest loss:  0.91726744\n",
      "Epoch:  406 \ttrain loss:  0.90897715 \ttest loss:  0.9172673\n",
      "Epoch:  411 \ttrain loss:  0.90897727 \ttest loss:  0.9172672\n",
      "Epoch:  416 \ttrain loss:  0.90897715 \ttest loss:  0.91726726\n",
      "Epoch:  421 \ttrain loss:  0.90897703 \ttest loss:  0.91726714\n",
      "Epoch:  426 \ttrain loss:  0.90897715 \ttest loss:  0.9172674\n",
      "Epoch:  431 \ttrain loss:  0.90897715 \ttest loss:  0.91726714\n",
      "Epoch:  436 \ttrain loss:  0.90897703 \ttest loss:  0.9172671\n",
      "Epoch:  441 \ttrain loss:  0.90897703 \ttest loss:  0.91726714\n",
      "Epoch:  446 \ttrain loss:  0.908977 \ttest loss:  0.91726714\n",
      "Epoch:  451 \ttrain loss:  0.90897703 \ttest loss:  0.9172671\n",
      "Epoch:  456 \ttrain loss:  0.908977 \ttest loss:  0.91726714\n",
      "Epoch:  461 \ttrain loss:  0.9089769 \ttest loss:  0.9172669\n",
      "Epoch:  466 \ttrain loss:  0.90897703 \ttest loss:  0.9172669\n",
      "Epoch:  471 \ttrain loss:  0.9089769 \ttest loss:  0.91726714\n",
      "Epoch:  476 \ttrain loss:  0.90897703 \ttest loss:  0.9172671\n",
      "Epoch:  481 \ttrain loss:  0.90897703 \ttest loss:  0.917267\n",
      "Epoch:  486 \ttrain loss:  0.908977 \ttest loss:  0.9172669\n",
      "Epoch:  491 \ttrain loss:  0.908977 \ttest loss:  0.917267\n",
      "Epoch:  496 \ttrain loss:  0.9089769 \ttest loss:  0.917267\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "reg = 1e-4\n",
    "\n",
    "global_mean = tf.placeholder(tf.float32, shape=[])\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "pu = tf.Variable(tf.random_normal([n_users, n_factors], 0.0, 0.01))\n",
    "qi = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "ratings = tf.placeholder(tf.int32, shape=[None])\n",
    "indices = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "\n",
    "dot = tf.matmul(pu, qi, transpose_b=True)\n",
    "bias_user_index = tf.gather(indices, 0, axis=1)\n",
    "bias_item_index = tf.gather(indices, 1, axis=1)\n",
    "bias_user = tf.nn.embedding_lookup(bu, bias_user_index)\n",
    "bias_item = tf.nn.embedding_lookup(bi, bias_item_index)\n",
    "pred = global_mean + bias_user + bias_item + tf.gather_nd(dot, indices)\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(tf.cast(ratings, tf.float32), pred))))\n",
    "\n",
    "\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(tf.cast(ratings, tf.float32), \n",
    "#                                                   tf.gather_nd(dot, indices)))))\n",
    "\n",
    "\n",
    "reg_pu = tf.contrib.layers.l2_regularizer(reg)(pu)\n",
    "reg_qi = tf.contrib.layers.l2_regularizer(reg)(qi)\n",
    "reg_bu = tf.contrib.layers.l2_regularizer(reg)(bu)\n",
    "reg_bi = tf.contrib.layers.l2_regularizer(reg)(bi)\n",
    "total_loss = tf.add_n([loss, reg_pu, reg_qi, reg_bu, reg_bi])\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "training_op = optimizer.minimize(total_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "step = 5\n",
    "for epoch in range(n_epochs):   # batch\n",
    "    sess.run(training_op, feed_dict={ratings: train_ratings, \n",
    "                                     indices: train_indices, \n",
    "                                     global_mean: np.mean(train_ratings)})\n",
    "    if epoch % step == 0:\n",
    "        train_loss = loss.eval(feed_dict={ratings: train_ratings, \n",
    "                                          indices: train_indices,\n",
    "                                          global_mean: np.mean(train_ratings)})\n",
    "        test_loss = loss.eval(feed_dict={ratings: test_ratings, \n",
    "                                         indices: test_indices, \n",
    "                                         global_mean: np.mean(train_ratings)})\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttrain loss: \", train_loss, \"\\ttest loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SVD++ - map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.array([train_user_indices, train_item_indices]).T\n",
    "test_indices = np.array([test_user_indices, test_item_indices]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "\n",
    "train_data = np.zeros((n_users, n_items), dtype=np.int32)\n",
    "for u in data:\n",
    "    u_items = list(data[u].keys())\n",
    "    train_data[u] = np.array(u_items + [0] * (n_items - len(u_items)))\n",
    "\n",
    "def nu_func(user):\n",
    "    u_items = tf.nn.embedding_lookup(pseudo_data, user)\n",
    "    zero = tf.constant(0, dtype=tf.int32)\n",
    "    mask = tf.not_equal(u_items, zero)\n",
    "    u_items_mask = tf.boolean_mask(u_items, mask)\n",
    "    return tf.reduce_sum(tf.gather(yj, u_items_mask), axis=0) / tf.sqrt(tf.cast(tf.size(u_items_mask), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before time:  6.2889463901519775\n",
      "Epoch:  1 training time:  139.17226481437683\n",
      "evaluate time:  0.8897066116333008\n",
      "Epoch:  1 \ttrain loss:  1.1100589 \ttest loss:  1.110304\n",
      "\n",
      "Epoch:  2 training time:  150.10314011573792\n",
      "evaluate time:  0.8937220573425293\n",
      "Epoch:  2 \ttrain loss:  1.1059463 \ttest loss:  1.1059988\n",
      "\n",
      "Epoch:  3 training time:  147.465482711792\n",
      "evaluate time:  0.991757869720459\n",
      "Epoch:  3 \ttrain loss:  1.1031151 \ttest loss:  1.1031557\n",
      "\n",
      "Epoch:  4 training time:  150.2542712688446\n",
      "evaluate time:  1.0497841835021973\n",
      "Epoch:  4 \ttrain loss:  1.1011149 \ttest loss:  1.1011428\n",
      "\n",
      "Epoch:  5 training time:  148.20391535758972\n",
      "evaluate time:  0.9735527038574219\n",
      "Epoch:  5 \ttrain loss:  1.0995452 \ttest loss:  1.0995774\n",
      "\n",
      "Epoch:  6 training time:  150.10250663757324\n",
      "evaluate time:  0.9864838123321533\n",
      "Epoch:  6 \ttrain loss:  1.0982659 \ttest loss:  1.0983186\n",
      "\n",
      "Epoch:  7 training time:  149.47698211669922\n",
      "evaluate time:  0.880845308303833\n",
      "Epoch:  7 \ttrain loss:  1.0972111 \ttest loss:  1.0973097\n",
      "\n",
      "Epoch:  8 training time:  140.30196738243103\n",
      "evaluate time:  0.8886721134185791\n",
      "Epoch:  8 \ttrain loss:  1.0964422 \ttest loss:  1.0965675\n",
      "\n",
      "Epoch:  9 training time:  137.23329186439514\n",
      "evaluate time:  0.931318998336792\n",
      "Epoch:  9 \ttrain loss:  1.0959737 \ttest loss:  1.0961094\n",
      "\n",
      "Epoch:  10 training time:  146.58544993400574\n",
      "evaluate time:  1.1935982704162598\n",
      "Epoch:  10 \ttrain loss:  1.0957836 \ttest loss:  1.095896\n",
      "\n",
      "Epoch:  11 training time:  146.26656198501587\n",
      "evaluate time:  1.0187947750091553\n",
      "Epoch:  11 \ttrain loss:  1.0957359 \ttest loss:  1.0958571\n",
      "\n",
      "Epoch:  12 training time:  143.59138655662537\n",
      "evaluate time:  0.895190954208374\n",
      "Epoch:  12 \ttrain loss:  1.0958222 \ttest loss:  1.095906\n",
      "\n",
      "Epoch:  13 training time:  138.1101405620575\n",
      "evaluate time:  0.8957037925720215\n",
      "Epoch:  13 \ttrain loss:  1.0958806 \ttest loss:  1.0959562\n",
      "\n",
      "Epoch:  14 training time:  137.58231210708618\n",
      "evaluate time:  0.8930206298828125\n",
      "Epoch:  14 \ttrain loss:  1.0958797 \ttest loss:  1.0959414\n",
      "\n",
      "Epoch:  15 training time:  138.38656497001648\n",
      "evaluate time:  0.8947417736053467\n",
      "Epoch:  15 \ttrain loss:  1.0957723 \ttest loss:  1.0958518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "user_list = np.array(list(data.keys()), dtype=np.int32)\n",
    "\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "reg = 0.01\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "ratings = tf.placeholder(tf.int32, shape=[None])\n",
    "indices = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "pseudo_data = tf.placeholder(tf.int32, shape=[n_users, n_items])  ###### max_length\n",
    "global_mean = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "pu = tf.Variable(tf.random_normal([n_users, n_factors], 0.0, 0.01))\n",
    "qi = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "yj = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "nu = tf.map_fn(nu_func, user_list, dtype=tf.float32)\n",
    "\n",
    "pn = pu + nu\n",
    "dot = tf.matmul(pn, qi, transpose_b=True)   ### + nu\n",
    "bias_user_index = tf.gather(indices, 0, axis=1)\n",
    "bias_item_index = tf.gather(indices, 1, axis=1)\n",
    "bias_user = tf.nn.embedding_lookup(bu, bias_user_index)\n",
    "bias_item = tf.nn.embedding_lookup(bi, bias_item_index)\n",
    "pred = global_mean + bias_user + bias_item + tf.gather_nd(dot, indices)\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(tf.cast(ratings, tf.float32), pred))))\n",
    "\n",
    "reg_pu = tf.contrib.layers.l2_regularizer(reg)(pu)\n",
    "reg_qi = tf.contrib.layers.l2_regularizer(reg)(qi)\n",
    "reg_bu = tf.contrib.layers.l2_regularizer(reg)(bu)\n",
    "reg_bi = tf.contrib.layers.l2_regularizer(reg)(bi)\n",
    "reg_yj = tf.contrib.layers.l2_regularizer(reg)(yj)\n",
    "total_loss = tf.add_n([loss, reg_pu, reg_qi, reg_bu, reg_bi, reg_yj])\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "training_op = optimizer.minimize(total_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "print(\"before time: \", time.time() - start_time)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):   # batch\n",
    "    t0 = time.time()\n",
    "    sess.run(training_op, feed_dict={ratings: train_ratings, \n",
    "                                     indices: train_indices, \n",
    "                                     global_mean: np.mean(train_ratings), \n",
    "                                     pseudo_data: train_data})\n",
    "    print(\"Epoch: \", epoch + 1, \"training time: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % display_step == 0:\n",
    "        t1 = time.time()\n",
    "        train_loss = loss.eval(feed_dict={ratings: train_ratings, \n",
    "                                          indices: train_indices,\n",
    "                                          global_mean: np.mean(train_ratings), \n",
    "                                          pseudo_data: train_data})\n",
    "        test_loss = loss.eval(feed_dict={ratings: test_ratings, \n",
    "                                         indices: test_indices, \n",
    "                                         global_mean: np.mean(train_ratings), \n",
    "                                         pseudo_data: train_data})\n",
    "        print(\"evaluate time: \", time.time() - t1)\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttrain loss: \", train_loss, \"\\ttest loss: \", test_loss)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before time:  5.525773286819458\n",
      "nu:  [[-0.0057214  -0.00065996 -0.00517013]\n",
      " [ 0.00721052  0.00029257  0.00556136]\n",
      " [ 0.007543    0.00088455 -0.0076777 ]]\n",
      "nu:  [[-4.7351066e-03  2.7604483e-04 -4.1846763e-03]\n",
      " [ 6.2189465e-03  5.3497328e-04  4.5721321e-03]\n",
      " [ 6.5523842e-03  1.7850427e-05 -6.6848118e-03]]\n",
      "nu:  [[-0.00376259  0.00097399 -0.00321155]\n",
      " [ 0.00523736  0.00028469  0.00359729]\n",
      " [ 0.00557088 -0.0006008  -0.00570177]]\n",
      "nu:  [[-2.8149788e-03  1.1397188e-03 -2.2780269e-03]\n",
      " [ 4.2720060e-03 -9.1242313e-05  2.6469096e-03]\n",
      " [ 4.6002800e-03 -1.0864055e-03 -4.7363606e-03]]\n",
      "nu:  [[-0.00191121  0.00088591 -0.00138573]\n",
      " [ 0.00332929 -0.00032933  0.00173388]\n",
      " [ 0.00365379 -0.00121142 -0.00379023]]\n",
      "nu:  [[-0.00105588  0.00048513 -0.00055325]\n",
      " [ 0.00241964 -0.00034645  0.00087357]\n",
      " [ 0.00273687 -0.00107057 -0.00287164]]\n",
      "nu:  [[-2.5976484e-04  6.5703556e-05  2.0307174e-04]\n",
      " [ 1.5534930e-03 -1.9178115e-04  8.3973166e-05]\n",
      " [ 1.8597220e-03 -7.6582358e-04 -1.9909653e-03]]\n",
      "nu:  [[ 4.5194512e-04 -3.2860384e-04  8.6194574e-04]\n",
      " [ 7.4285455e-04  9.4744057e-05 -6.1565678e-04]\n",
      " [ 1.0347203e-03 -3.8588472e-04 -1.1581501e-03]]\n",
      "nu:  [[ 1.0666935e-03 -6.1164435e-04  1.4129134e-03]\n",
      " [-1.4099642e-06  3.0330932e-04 -1.2074218e-03]\n",
      " [ 2.7157582e-04  1.0447431e-05 -3.8317323e-04]]\n",
      "nu:  [[ 0.00156194 -0.00073664  0.00183941]\n",
      " [-0.0006656   0.00035411 -0.00167881]\n",
      " [-0.00041664  0.00037246  0.00032164]]\n",
      "nu:  [[ 0.00193597 -0.00070521  0.00213138]\n",
      " [-0.00123881  0.00025505 -0.00202313]\n",
      " [-0.00101958  0.00063565  0.00094571]]\n",
      "nu:  [[ 2.1868714e-03 -5.5669399e-04  2.2928512e-03]\n",
      " [-1.7123122e-03  5.8176855e-05 -2.2398105e-03]\n",
      " [-1.5293423e-03  7.6757785e-04  1.4774171e-03]]\n",
      "nu:  [[ 0.00231896 -0.00034716  0.00233485]\n",
      " [-0.00208169 -0.00014122 -0.00233719]\n",
      " [-0.00194003  0.00077241  0.00190896]]\n",
      "nu:  [[ 2.3437950e-03 -9.9688128e-05  2.2722476e-03]\n",
      " [-2.3461934e-03 -2.6505196e-04 -2.3266440e-03]\n",
      " [-2.2478881e-03  6.7012053e-04  2.2387400e-03]]\n",
      "nu:  [[ 0.00227305  0.00013954  0.00211955]\n",
      " [-0.00250842 -0.00027041 -0.00222229]\n",
      " [-0.00245552  0.00048066  0.00246717]]\n",
      "nu:  [[ 0.00212096  0.00032129  0.00189544]\n",
      " [-0.00257515 -0.00017587 -0.00203949]\n",
      " [-0.00256819  0.00023992  0.00259821]]\n",
      "nu:  [[ 1.90224894e-03  4.20140685e-04  1.61524548e-03]\n",
      " [-2.55480665e-03 -2.40392546e-05 -1.79402111e-03]\n",
      " [-2.59654620e-03 -1.33434805e-05  2.63826153e-03]]\n",
      "nu:  [[ 0.00163145  0.00043642  0.00129462]\n",
      " [-0.00245639  0.00012529 -0.0015017 ]\n",
      " [-0.00254472 -0.00023964  0.00259578]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "user_list = np.array(list(data.keys()), dtype=np.int32)\n",
    "# user_list = np.arange(n_users, dtype=np.int32)\n",
    "\n",
    "n_epochs = 500\n",
    "lr = 0.001\n",
    "reg = 0.01\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "ratings = tf.placeholder(tf.int32, shape=[None])\n",
    "indices = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "pseudo_data = tf.placeholder(tf.int32, shape=[n_users, n_items])  ###### max_length\n",
    "global_mean = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "pu = tf.Variable(tf.random_normal([n_users, n_factors], 0.0, 0.01))\n",
    "qi = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "yj = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "nu = tf.map_fn(nu_func, user_list, dtype=tf.float32)\n",
    "\n",
    "pn = pu + nu\n",
    "dot = tf.matmul(pn, qi, transpose_b=True)   ### + nu\n",
    "bias_user_index = tf.gather(indices, 0, axis=1)\n",
    "bias_item_index = tf.gather(indices, 1, axis=1)\n",
    "bias_user = tf.nn.embedding_lookup(bu, bias_user_index)\n",
    "bias_item = tf.nn.embedding_lookup(bi, bias_item_index)\n",
    "pred = global_mean + bias_user + bias_item + tf.gather_nd(dot, indices)\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(tf.cast(ratings, tf.float32), pred))))\n",
    "\n",
    "reg_pu = tf.contrib.layers.l2_regularizer(reg)(pu)\n",
    "reg_qi = tf.contrib.layers.l2_regularizer(reg)(qi)\n",
    "reg_bu = tf.contrib.layers.l2_regularizer(reg)(bu)\n",
    "reg_bi = tf.contrib.layers.l2_regularizer(reg)(bi)\n",
    "reg_yj = tf.contrib.layers.l2_regularizer(reg)(yj)\n",
    "total_loss = tf.add_n([loss, reg_pu, reg_qi, reg_bu, reg_bi, reg_yj])\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "training_op = optimizer.minimize(total_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "print(\"before time: \", time.time() - start_time)\n",
    "\n",
    "batch_size = 10000\n",
    "for epoch in range(n_epochs):   # batch\n",
    "    \n",
    "    t0 = time.time()\n",
    "    n_batches = len(train_ratings) // batch_size\n",
    "    for i in range(n_batches):\n",
    "        end = min((i+1) * batch_size, len(train_ratings))\n",
    "        train_batch_ratings = train_ratings[i * batch_size : end]\n",
    "        train_batch_indices = train_indices[i * batch_size : end]\n",
    "        sess.run(training_op, feed_dict={ratings: train_batch_ratings, \n",
    "                                         indices: train_batch_indices, \n",
    "                                         global_mean: np.mean(train_ratings), \n",
    "                                         pseudo_data: train_data})\n",
    "        \n",
    "        print(\"nu: \", pu.eval()[:3, :3])\n",
    "\n",
    "    print(\"Epoch: \", epoch + 1, \"training time: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % display_step == 0:\n",
    "        t1 = time.time()\n",
    "        train_loss = loss.eval(feed_dict={ratings: train_ratings, \n",
    "                                          indices: train_indices,\n",
    "                                          global_mean: np.mean(train_ratings), \n",
    "                                          pseudo_data: train_data})\n",
    "        test_loss = loss.eval(feed_dict={ratings: test_ratings, \n",
    "                                         indices: test_indices, \n",
    "                                         global_mean: np.mean(train_ratings), \n",
    "                                         pseudo_data: train_data})\n",
    "        print(\"evaluate time: \", time.time() - t1)\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttrain loss: \", train_loss, \"\\ttest loss: \", test_loss)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD++ - list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.array([train_user_indices, train_item_indices]).T\n",
    "test_indices = np.array([test_user_indices, test_item_indices]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before time:  208.99212789535522\n",
      "Epoch:  1 \ttrain loss:  1.1095084 \ttest loss:  1.1097265\n",
      "Epoch:  6 \ttrain loss:  0.9734519 \ttest loss:  0.98090374\n",
      "Epoch:  11 \ttrain loss:  0.9440581 \ttest loss:  0.95206994\n",
      "Epoch:  16 \ttrain loss:  0.92387855 \ttest loss:  0.93359923\n",
      "Epoch:  21 \ttrain loss:  0.90651083 \ttest loss:  0.91885835\n",
      "Epoch:  26 \ttrain loss:  0.8933545 \ttest loss:  0.907738\n",
      "Epoch:  31 \ttrain loss:  0.88307565 \ttest loss:  0.8992146\n",
      "Epoch:  36 \ttrain loss:  0.8759572 \ttest loss:  0.8935039\n",
      "Epoch:  41 \ttrain loss:  0.8710839 \ttest loss:  0.88962024\n",
      "Epoch:  46 \ttrain loss:  0.8676232 \ttest loss:  0.88676316\n",
      "Epoch:  51 \ttrain loss:  0.86540294 \ttest loss:  0.8850033\n",
      "Epoch:  56 \ttrain loss:  0.8639244 \ttest loss:  0.88378817\n",
      "Epoch:  61 \ttrain loss:  0.8625998 \ttest loss:  0.88279444\n",
      "Epoch:  66 \ttrain loss:  0.86126405 \ttest loss:  0.8819406\n",
      "Epoch:  71 \ttrain loss:  0.85990965 \ttest loss:  0.88110894\n",
      "Epoch:  76 \ttrain loss:  0.858642 \ttest loss:  0.8803308\n",
      "Epoch:  81 \ttrain loss:  0.8575475 \ttest loss:  0.8796292\n",
      "Epoch:  86 \ttrain loss:  0.8566589 \ttest loss:  0.8790667\n",
      "Epoch:  91 \ttrain loss:  0.8559884 \ttest loss:  0.8786224\n",
      "Epoch:  96 \ttrain loss:  0.8554941 \ttest loss:  0.878278\n",
      "Epoch:  101 \ttrain loss:  0.85512173 \ttest loss:  0.87801677\n",
      "Epoch:  106 \ttrain loss:  0.8548217 \ttest loss:  0.8777919\n",
      "Epoch:  111 \ttrain loss:  0.8545568 \ttest loss:  0.8775897\n",
      "Epoch:  116 \ttrain loss:  0.8543104 \ttest loss:  0.8774096\n",
      "Epoch:  121 \ttrain loss:  0.8540723 \ttest loss:  0.8772423\n",
      "Epoch:  126 \ttrain loss:  0.8538401 \ttest loss:  0.87708783\n",
      "Epoch:  131 \ttrain loss:  0.8536193 \ttest loss:  0.8769504\n",
      "Epoch:  136 \ttrain loss:  0.85341114 \ttest loss:  0.8768225\n",
      "Epoch:  141 \ttrain loss:  0.85321903 \ttest loss:  0.87670827\n",
      "Epoch:  146 \ttrain loss:  0.85305166 \ttest loss:  0.87660915\n",
      "Epoch:  151 \ttrain loss:  0.85290825 \ttest loss:  0.8765241\n",
      "Epoch:  156 \ttrain loss:  0.85278964 \ttest loss:  0.87645483\n",
      "Epoch:  161 \ttrain loss:  0.8526935 \ttest loss:  0.87639695\n",
      "Epoch:  166 \ttrain loss:  0.8526164 \ttest loss:  0.8763497\n",
      "Epoch:  171 \ttrain loss:  0.8525517 \ttest loss:  0.87631226\n",
      "Epoch:  176 \ttrain loss:  0.8524972 \ttest loss:  0.8762782\n",
      "Epoch:  181 \ttrain loss:  0.85244745 \ttest loss:  0.8762479\n",
      "Epoch:  186 \ttrain loss:  0.8524009 \ttest loss:  0.87622255\n",
      "Epoch:  191 \ttrain loss:  0.85235953 \ttest loss:  0.876193\n",
      "Epoch:  196 \ttrain loss:  0.85249174 \ttest loss:  0.87641627\n",
      "Epoch:  201 \ttrain loss:  0.8525048 \ttest loss:  0.8763119\n",
      "Epoch:  206 \ttrain loss:  0.85264397 \ttest loss:  0.87654936\n",
      "Epoch:  211 \ttrain loss:  0.85241616 \ttest loss:  0.87632173\n",
      "Epoch:  216 \ttrain loss:  0.85229844 \ttest loss:  0.8761858\n",
      "Epoch:  221 \ttrain loss:  0.85227364 \ttest loss:  0.876177\n",
      "Epoch:  226 \ttrain loss:  0.8521414 \ttest loss:  0.8760784\n",
      "Epoch:  231 \ttrain loss:  0.8520707 \ttest loss:  0.87604666\n",
      "Epoch:  236 \ttrain loss:  0.85204625 \ttest loss:  0.8760569\n",
      "Epoch:  241 \ttrain loss:  0.852024 \ttest loss:  0.8760465\n",
      "Epoch:  246 \ttrain loss:  0.8520008 \ttest loss:  0.8760307\n",
      "Epoch:  251 \ttrain loss:  0.8519831 \ttest loss:  0.8760213\n",
      "Epoch:  256 \ttrain loss:  0.8519673 \ttest loss:  0.8760115\n",
      "Epoch:  261 \ttrain loss:  0.8519524 \ttest loss:  0.87600285\n",
      "Epoch:  266 \ttrain loss:  0.8519401 \ttest loss:  0.8759945\n",
      "Epoch:  271 \ttrain loss:  0.8519287 \ttest loss:  0.875986\n",
      "Epoch:  276 \ttrain loss:  0.851921 \ttest loss:  0.8759798\n",
      "Epoch:  281 \ttrain loss:  0.851914 \ttest loss:  0.87597525\n",
      "Epoch:  286 \ttrain loss:  0.85190594 \ttest loss:  0.8759717\n",
      "Epoch:  291 \ttrain loss:  0.8518945 \ttest loss:  0.8759685\n",
      "Epoch:  296 \ttrain loss:  0.85188615 \ttest loss:  0.87596595\n",
      "Epoch:  301 \ttrain loss:  0.85188097 \ttest loss:  0.8759617\n",
      "Epoch:  306 \ttrain loss:  0.85187227 \ttest loss:  0.8759592\n",
      "Epoch:  311 \ttrain loss:  0.851866 \ttest loss:  0.8759565\n",
      "Epoch:  316 \ttrain loss:  0.85186094 \ttest loss:  0.8759526\n",
      "Epoch:  321 \ttrain loss:  0.85185 \ttest loss:  0.8759538\n",
      "Epoch:  326 \ttrain loss:  0.85187984 \ttest loss:  0.8759571\n",
      "Epoch:  331 \ttrain loss:  0.85231483 \ttest loss:  0.8765057\n",
      "Epoch:  336 \ttrain loss:  0.8522299 \ttest loss:  0.8762864\n",
      "Epoch:  341 \ttrain loss:  0.85224444 \ttest loss:  0.8763924\n",
      "Epoch:  346 \ttrain loss:  0.8520924 \ttest loss:  0.8762296\n",
      "Epoch:  351 \ttrain loss:  0.8519504 \ttest loss:  0.87602156\n",
      "Epoch:  356 \ttrain loss:  0.85191095 \ttest loss:  0.87595505\n",
      "Epoch:  361 \ttrain loss:  0.8518512 \ttest loss:  0.8759215\n",
      "Epoch:  366 \ttrain loss:  0.8518541 \ttest loss:  0.8759805\n",
      "Epoch:  371 \ttrain loss:  0.8518629 \ttest loss:  0.8760017\n",
      "Epoch:  376 \ttrain loss:  0.851849 \ttest loss:  0.87598175\n",
      "Epoch:  381 \ttrain loss:  0.85182476 \ttest loss:  0.8759534\n",
      "Epoch:  386 \ttrain loss:  0.8518017 \ttest loss:  0.87592757\n",
      "Epoch:  391 \ttrain loss:  0.8517897 \ttest loss:  0.8759201\n",
      "Epoch:  396 \ttrain loss:  0.8517882 \ttest loss:  0.87591755\n",
      "Epoch:  401 \ttrain loss:  0.8517903 \ttest loss:  0.87592006\n",
      "Epoch:  406 \ttrain loss:  0.851787 \ttest loss:  0.87592006\n",
      "Epoch:  411 \ttrain loss:  0.85178286 \ttest loss:  0.8759184\n",
      "Epoch:  416 \ttrain loss:  0.8517862 \ttest loss:  0.8759217\n",
      "Epoch:  421 \ttrain loss:  0.85177815 \ttest loss:  0.8759202\n",
      "Epoch:  426 \ttrain loss:  0.8517904 \ttest loss:  0.87592727\n",
      "Epoch:  431 \ttrain loss:  0.8517781 \ttest loss:  0.87593377\n",
      "Epoch:  436 \ttrain loss:  0.8521139 \ttest loss:  0.8761973\n",
      "Epoch:  441 \ttrain loss:  0.85238427 \ttest loss:  0.87649304\n",
      "Epoch:  446 \ttrain loss:  0.8522052 \ttest loss:  0.87631524\n",
      "Epoch:  451 \ttrain loss:  0.8520323 \ttest loss:  0.87616336\n",
      "Epoch:  456 \ttrain loss:  0.8518969 \ttest loss:  0.87604517\n",
      "Epoch:  461 \ttrain loss:  0.85183656 \ttest loss:  0.87598056\n",
      "Epoch:  466 \ttrain loss:  0.8518804 \ttest loss:  0.87601227\n",
      "Epoch:  471 \ttrain loss:  0.8518575 \ttest loss:  0.8759965\n",
      "Epoch:  476 \ttrain loss:  0.85178936 \ttest loss:  0.87594134\n",
      "Epoch:  481 \ttrain loss:  0.8517651 \ttest loss:  0.8759239\n",
      "Epoch:  486 \ttrain loss:  0.8517625 \ttest loss:  0.87591416\n",
      "Epoch:  491 \ttrain loss:  0.8517852 \ttest loss:  0.8759312\n",
      "Epoch:  496 \ttrain loss:  0.8517595 \ttest loss:  0.87590927\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "reg = 1e-4\n",
    "display_step = 5\n",
    "\n",
    "global_mean = tf.placeholder(tf.float32, shape=[])\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "pu = tf.Variable(tf.random_normal([n_users, n_factors], 0.0, 0.01))\n",
    "qi = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "yj = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))  ####### yj\n",
    "# nu_sqrt = np.sqrt(len(data[u]))\n",
    "# Iu = [tf.reduce_sum(tf.gather(yj, list(data[u].keys())), axis=0) / np.sqrt(len(data[u])) for u in data.keys()]\n",
    "# nu = tf.convert_to_tensor(Iu, dtype=tf.float32)\n",
    "nu = [tf.reduce_sum(tf.gather(yj, list(data[u].keys())), axis=0) / np.sqrt(len(data[u])) for u in data.keys()]\n",
    "\n",
    "ratings = tf.placeholder(tf.int32, shape=[None])\n",
    "indices = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "\n",
    "pn = pu + nu\n",
    "dot = tf.matmul(pn, qi, transpose_b=True)   ### + nu\n",
    "bias_user_index = tf.gather(indices, 0, axis=1)\n",
    "bias_item_index = tf.gather(indices, 1, axis=1)\n",
    "bias_user = tf.nn.embedding_lookup(bu, bias_user_index)\n",
    "bias_item = tf.nn.embedding_lookup(bi, bias_item_index)\n",
    "pred = global_mean + bias_user + bias_item + tf.gather_nd(dot, indices)\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(tf.cast(ratings, tf.float32), pred))))\n",
    "\n",
    "\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(tf.cast(ratings, tf.float32), \n",
    "#                                                   tf.gather_nd(dot, indices)))))\n",
    "\n",
    "\n",
    "reg_pu = tf.contrib.layers.l2_regularizer(reg)(pu)\n",
    "reg_qi = tf.contrib.layers.l2_regularizer(reg)(qi)\n",
    "reg_bu = tf.contrib.layers.l2_regularizer(reg)(bu)\n",
    "reg_bi = tf.contrib.layers.l2_regularizer(reg)(bi)\n",
    "reg_yj = tf.contrib.layers.l2_regularizer(reg)(yj)\n",
    "total_loss = tf.add_n([loss, reg_pu, reg_qi, reg_bu, reg_bi, reg_yj])\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "training_op = optimizer.minimize(total_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "print(\"before time: \", time.time() - start_time)\n",
    "\n",
    "for epoch in range(n_epochs):   # batch\n",
    "    sess.run(training_op, feed_dict={ratings: train_ratings, \n",
    "                                     indices: train_indices, \n",
    "                                     global_mean: np.mean(train_ratings)})\n",
    "    \n",
    "    if epoch % display_step == 0:\n",
    "        train_loss = loss.eval(feed_dict={ratings: train_ratings, \n",
    "                                          indices: train_indices,\n",
    "                                          global_mean: np.mean(train_ratings)})\n",
    "        test_loss = loss.eval(feed_dict={ratings: test_ratings, \n",
    "                                         indices: test_indices, \n",
    "                                         global_mean: np.mean(train_ratings)})\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttrain loss: \", train_loss, \"\\ttest loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before time:  5.503317832946777\n",
      "Epoch:  1 \ttrain loss:  1.1084875 \ttest loss:  1.1087164\n",
      "Epoch:  6 \ttrain loss:  0.9728028 \ttest loss:  0.9801653\n",
      "Epoch:  11 \ttrain loss:  0.9436891 \ttest loss:  0.95172304\n",
      "Epoch:  16 \ttrain loss:  0.92393833 \ttest loss:  0.9336424\n",
      "Epoch:  21 \ttrain loss:  0.906229 \ttest loss:  0.9189427\n",
      "Epoch:  26 \ttrain loss:  0.89356774 \ttest loss:  0.9078903\n",
      "Epoch:  31 \ttrain loss:  0.8826899 \ttest loss:  0.89891696\n",
      "Epoch:  36 \ttrain loss:  0.8748378 \ttest loss:  0.8927291\n",
      "Epoch:  41 \ttrain loss:  0.86970496 \ttest loss:  0.8887451\n",
      "Epoch:  46 \ttrain loss:  0.8666014 \ttest loss:  0.8860921\n",
      "Epoch:  51 \ttrain loss:  0.86459994 \ttest loss:  0.88449574\n",
      "Epoch:  56 \ttrain loss:  0.8633052 \ttest loss:  0.8833991\n",
      "Epoch:  61 \ttrain loss:  0.8620211 \ttest loss:  0.8825423\n",
      "Epoch:  66 \ttrain loss:  0.86076546 \ttest loss:  0.88172114\n",
      "Epoch:  71 \ttrain loss:  0.8595821 \ttest loss:  0.8809589\n",
      "Epoch:  76 \ttrain loss:  0.858482 \ttest loss:  0.88021207\n",
      "Epoch:  81 \ttrain loss:  0.8575063 \ttest loss:  0.87956583\n",
      "Epoch:  86 \ttrain loss:  0.85670173 \ttest loss:  0.8790313\n",
      "Epoch:  91 \ttrain loss:  0.8560625 \ttest loss:  0.8786191\n",
      "Epoch:  96 \ttrain loss:  0.85555613 \ttest loss:  0.8782778\n",
      "Epoch:  101 \ttrain loss:  0.8551453 \ttest loss:  0.8780017\n",
      "Epoch:  106 \ttrain loss:  0.85478383 \ttest loss:  0.8777544\n",
      "Epoch:  111 \ttrain loss:  0.8544533 \ttest loss:  0.8775335\n",
      "Epoch:  116 \ttrain loss:  0.85415566 \ttest loss:  0.87733704\n",
      "Epoch:  121 \ttrain loss:  0.85388666 \ttest loss:  0.87716514\n",
      "Epoch:  126 \ttrain loss:  0.85364676 \ttest loss:  0.8770102\n",
      "Epoch:  131 \ttrain loss:  0.85343105 \ttest loss:  0.87687427\n",
      "Epoch:  136 \ttrain loss:  0.8532403 \ttest loss:  0.8767553\n",
      "Epoch:  141 \ttrain loss:  0.85307175 \ttest loss:  0.87664986\n",
      "Epoch:  146 \ttrain loss:  0.85292506 \ttest loss:  0.87655675\n",
      "Epoch:  151 \ttrain loss:  0.85279924 \ttest loss:  0.87647927\n",
      "Epoch:  156 \ttrain loss:  0.85269177 \ttest loss:  0.8764119\n",
      "Epoch:  161 \ttrain loss:  0.8526012 \ttest loss:  0.87635535\n",
      "Epoch:  166 \ttrain loss:  0.85252565 \ttest loss:  0.87630916\n",
      "Epoch:  171 \ttrain loss:  0.85245925 \ttest loss:  0.87627065\n",
      "Epoch:  176 \ttrain loss:  0.85240054 \ttest loss:  0.8762365\n",
      "Epoch:  181 \ttrain loss:  0.8523474 \ttest loss:  0.87620646\n",
      "Epoch:  186 \ttrain loss:  0.8523017 \ttest loss:  0.8761771\n",
      "Epoch:  191 \ttrain loss:  0.852387 \ttest loss:  0.87632895\n",
      "Epoch:  196 \ttrain loss:  0.852545 \ttest loss:  0.8763842\n",
      "Epoch:  201 \ttrain loss:  0.85252315 \ttest loss:  0.8764137\n",
      "Epoch:  206 \ttrain loss:  0.85244316 \ttest loss:  0.8763512\n",
      "Epoch:  211 \ttrain loss:  0.85221773 \ttest loss:  0.8761681\n",
      "Epoch:  216 \ttrain loss:  0.8522036 \ttest loss:  0.87615454\n",
      "Epoch:  221 \ttrain loss:  0.8521268 \ttest loss:  0.87610275\n",
      "Epoch:  226 \ttrain loss:  0.85204226 \ttest loss:  0.8760448\n",
      "Epoch:  231 \ttrain loss:  0.8520046 \ttest loss:  0.8760318\n",
      "Epoch:  236 \ttrain loss:  0.85198027 \ttest loss:  0.8760141\n",
      "Epoch:  241 \ttrain loss:  0.8519605 \ttest loss:  0.8760074\n",
      "Epoch:  246 \ttrain loss:  0.85194564 \ttest loss:  0.87600505\n",
      "Epoch:  251 \ttrain loss:  0.8519278 \ttest loss:  0.8759922\n",
      "Epoch:  256 \ttrain loss:  0.85191554 \ttest loss:  0.8759829\n",
      "Epoch:  261 \ttrain loss:  0.8519062 \ttest loss:  0.8759767\n",
      "Epoch:  266 \ttrain loss:  0.8518972 \ttest loss:  0.87597364\n",
      "Epoch:  271 \ttrain loss:  0.8518886 \ttest loss:  0.8759707\n",
      "Epoch:  276 \ttrain loss:  0.85188043 \ttest loss:  0.87596583\n",
      "Epoch:  281 \ttrain loss:  0.85187036 \ttest loss:  0.8759605\n",
      "Epoch:  286 \ttrain loss:  0.85186213 \ttest loss:  0.8759572\n",
      "Epoch:  291 \ttrain loss:  0.85185766 \ttest loss:  0.8759545\n",
      "Epoch:  296 \ttrain loss:  0.8518503 \ttest loss:  0.8759517\n",
      "Epoch:  301 \ttrain loss:  0.8518434 \ttest loss:  0.87594855\n",
      "Epoch:  306 \ttrain loss:  0.8518407 \ttest loss:  0.8759468\n",
      "Epoch:  311 \ttrain loss:  0.8518324 \ttest loss:  0.8759475\n",
      "Epoch:  316 \ttrain loss:  0.85189277 \ttest loss:  0.875988\n",
      "Epoch:  321 \ttrain loss:  0.852593 \ttest loss:  0.87671137\n",
      "Epoch:  326 \ttrain loss:  0.8523036 \ttest loss:  0.8763553\n",
      "Epoch:  331 \ttrain loss:  0.8521977 \ttest loss:  0.87627083\n",
      "Epoch:  336 \ttrain loss:  0.85207385 \ttest loss:  0.8761651\n",
      "Epoch:  341 \ttrain loss:  0.85197794 \ttest loss:  0.8760914\n",
      "Epoch:  346 \ttrain loss:  0.8518946 \ttest loss:  0.8760082\n",
      "Epoch:  351 \ttrain loss:  0.85183585 \ttest loss:  0.8759424\n",
      "Epoch:  356 \ttrain loss:  0.85181415 \ttest loss:  0.87592626\n",
      "Epoch:  361 \ttrain loss:  0.8518152 \ttest loss:  0.87593406\n",
      "Epoch:  366 \ttrain loss:  0.8518229 \ttest loss:  0.87594396\n",
      "Epoch:  371 \ttrain loss:  0.8518157 \ttest loss:  0.87593836\n",
      "Epoch:  376 \ttrain loss:  0.85179806 \ttest loss:  0.87592655\n",
      "Epoch:  381 \ttrain loss:  0.85178846 \ttest loss:  0.87592274\n",
      "Epoch:  386 \ttrain loss:  0.8517866 \ttest loss:  0.8759226\n",
      "Epoch:  391 \ttrain loss:  0.85179293 \ttest loss:  0.8759277\n",
      "Epoch:  396 \ttrain loss:  0.8517889 \ttest loss:  0.8759257\n",
      "Epoch:  401 \ttrain loss:  0.85178286 \ttest loss:  0.875925\n",
      "Epoch:  406 \ttrain loss:  0.85178894 \ttest loss:  0.8759286\n",
      "Epoch:  411 \ttrain loss:  0.85178006 \ttest loss:  0.8759243\n",
      "Epoch:  416 \ttrain loss:  0.85181385 \ttest loss:  0.8759476\n",
      "Epoch:  421 \ttrain loss:  0.8518971 \ttest loss:  0.87604713\n",
      "Epoch:  426 \ttrain loss:  0.85256195 \ttest loss:  0.8765987\n",
      "Epoch:  431 \ttrain loss:  0.85218483 \ttest loss:  0.87624437\n",
      "Epoch:  436 \ttrain loss:  0.8519406 \ttest loss:  0.87603766\n",
      "Epoch:  441 \ttrain loss:  0.85188264 \ttest loss:  0.87600744\n",
      "Epoch:  446 \ttrain loss:  0.85186946 \ttest loss:  0.8760132\n",
      "Epoch:  451 \ttrain loss:  0.85181093 \ttest loss:  0.8759565\n",
      "Epoch:  456 \ttrain loss:  0.85185504 \ttest loss:  0.8759899\n",
      "Epoch:  461 \ttrain loss:  0.85179085 \ttest loss:  0.87592554\n",
      "Epoch:  466 \ttrain loss:  0.8517755 \ttest loss:  0.8759239\n",
      "Epoch:  471 \ttrain loss:  0.8517942 \ttest loss:  0.8759457\n",
      "Epoch:  476 \ttrain loss:  0.8517703 \ttest loss:  0.87591803\n",
      "Epoch:  481 \ttrain loss:  0.85176164 \ttest loss:  0.87591344\n",
      "Epoch:  486 \ttrain loss:  0.8517981 \ttest loss:  0.87595046\n",
      "Epoch:  491 \ttrain loss:  0.8518112 \ttest loss:  0.87595814\n",
      "Epoch:  496 \ttrain loss:  0.8522124 \ttest loss:  0.87631786\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "reg = 1e-4\n",
    "display_step = 5\n",
    "\n",
    "global_mean = tf.placeholder(tf.float32, shape=[])\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "pu = tf.Variable(tf.random_normal([n_users, n_factors], 0.0, 0.01))\n",
    "qi = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "yj = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "# nu = [tf.reduce_sum(tf.gather(yj, list(data[u].keys())), axis=0) / np.sqrt(len(data[u])) for u in data.keys()]\n",
    "\n",
    "N = [[] for u in range(n_users)]\n",
    "for u, i, in zip(train_user_indices, train_item_indices):\n",
    "    N[u].append(i)\n",
    "\n",
    "sparse = {'indices': [], 'values': []}\n",
    "for i, user in enumerate(N):\n",
    "    for j, item in enumerate(user):\n",
    "        sparse['indices'].append((i, j))\n",
    "        sparse['values'].append(item)\n",
    "        \n",
    "sparse['dense_shape'] = (n_users, n_items)\n",
    "implicit_feedback = tf.SparseTensor(**sparse)\n",
    "\n",
    "yjs = tf.nn.embedding_lookup_sparse(yj, implicit_feedback, sp_weights=None, combiner='sqrtn')\n",
    "nu = tf.gather(yjs, np.arange(n_users))\n",
    "\n",
    "'''\n",
    "ci = tf.nn.embedding_lookup(c, train_item_indices)\n",
    "cs = tf.nn.embedding_lookup_sparse(ci, implicit_feedback, sp_weights=None, combine='sqrtn')\n",
    "nu = tf.gather(cs, train_item_indices)\n",
    "'''\n",
    "\n",
    "ratings = tf.placeholder(tf.int32, shape=[None])\n",
    "indices = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "\n",
    "pn = pu + nu\n",
    "dot = tf.matmul(pn, qi, transpose_b=True)   ### + nu\n",
    "bias_user_index = tf.gather(indices, 0, axis=1)\n",
    "bias_item_index = tf.gather(indices, 1, axis=1)\n",
    "bias_user = tf.nn.embedding_lookup(bu, bias_user_index)\n",
    "bias_item = tf.nn.embedding_lookup(bi, bias_item_index)\n",
    "pred = global_mean + bias_user + bias_item + tf.gather_nd(dot, indices)\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(tf.cast(ratings, tf.float32), pred))))\n",
    "\n",
    "reg_pu = tf.contrib.layers.l2_regularizer(reg)(pu)\n",
    "reg_qi = tf.contrib.layers.l2_regularizer(reg)(qi)\n",
    "reg_bu = tf.contrib.layers.l2_regularizer(reg)(bu)\n",
    "reg_bi = tf.contrib.layers.l2_regularizer(reg)(bi)\n",
    "reg_yj = tf.contrib.layers.l2_regularizer(reg)(yj)\n",
    "total_loss = tf.add_n([loss, reg_pu, reg_qi, reg_bu, reg_bi, reg_yj])\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "training_op = optimizer.minimize(total_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "print(\"before time: \", time.time() - start_time)\n",
    "\n",
    "for epoch in range(n_epochs):   # batch\n",
    "    sess.run(training_op, feed_dict={ratings: train_ratings, \n",
    "                                     indices: train_indices, \n",
    "                                     global_mean: np.mean(train_ratings)})\n",
    "    \n",
    "    if epoch % display_step == 0:\n",
    "        train_loss = loss.eval(feed_dict={ratings: train_ratings, \n",
    "                                          indices: train_indices,\n",
    "                                          global_mean: np.mean(train_ratings)})\n",
    "        test_loss = loss.eval(feed_dict={ratings: test_ratings, \n",
    "                                         indices: test_indices, \n",
    "                                         global_mean: np.mean(train_ratings)})\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttrain loss: \", train_loss, \"\\ttest loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch list [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.array([train_user_indices, train_item_indices]).T\n",
    "test_indices = np.array([test_user_indices, test_item_indices]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before time:  208.64603281021118\n",
      "Epoch:  1 training time:  78.99554300308228\n",
      "evaluate time:  7.826488018035889\n",
      "Epoch:  1 \ttrain loss:  0.8932651 \ttest loss:  0.9069787\n",
      "\n",
      "Epoch:  2 training time:  48.82956886291504\n",
      "Epoch:  3 training time:  48.218790769577026\n",
      "Epoch:  4 training time:  48.840534687042236\n",
      "Epoch:  5 training time:  48.52321267127991\n",
      "Epoch:  6 training time:  50.79559254646301\n",
      "evaluate time:  0.4228475093841553\n",
      "Epoch:  6 \ttrain loss:  0.8731428 \ttest loss:  0.89099956\n",
      "\n",
      "Epoch:  7 training time:  47.72296142578125\n",
      "Epoch:  8 training time:  48.81165027618408\n",
      "Epoch:  9 training time:  48.78493094444275\n",
      "Epoch:  10 training time:  50.08138036727905\n",
      "Epoch:  11 training time:  50.19994759559631\n",
      "evaluate time:  0.4386272430419922\n",
      "Epoch:  11 \ttrain loss:  0.8755225 \ttest loss:  0.89458275\n",
      "\n",
      "Epoch:  12 training time:  49.42346549034119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56bd588e73d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m         sess.run(training_op, feed_dict={ratings: train_batch_ratings, \n\u001b[1;32m     64\u001b[0m                                          \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_batch_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                                          global_mean: np.mean(train_ratings)})\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#        print(\"nu: \", pu.eval()[:3, :3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "reg = 1e-4\n",
    "display_step = 5\n",
    "\n",
    "global_mean = tf.placeholder(tf.float32, shape=[])\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "pu = tf.Variable(tf.random_normal([n_users, n_factors], 0.0, 0.01))\n",
    "qi = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "yj = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))  ####### yj\n",
    "# nu_sqrt = np.sqrt(len(data[u]))\n",
    "# Iu = [tf.reduce_sum(tf.gather(yj, list(data[u].keys())), axis=0) / np.sqrt(len(data[u])) for u in data.keys()]\n",
    "# nu = tf.convert_to_tensor(Iu, dtype=tf.float32)\n",
    "nu = [tf.reduce_sum(tf.gather(yj, list(data[u].keys())), axis=0) / np.sqrt(len(data[u])) for u in data.keys()]\n",
    "\n",
    "ratings = tf.placeholder(tf.int32, shape=[None])\n",
    "indices = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "\n",
    "pn = pu + nu\n",
    "dot = tf.matmul(pn, qi, transpose_b=True)   ### + nu\n",
    "bias_user_index = tf.gather(indices, 0, axis=1)\n",
    "bias_item_index = tf.gather(indices, 1, axis=1)\n",
    "bias_user = tf.nn.embedding_lookup(bu, bias_user_index)\n",
    "bias_item = tf.nn.embedding_lookup(bi, bias_item_index)\n",
    "pred = global_mean + bias_user + bias_item + tf.gather_nd(dot, indices)\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(tf.cast(ratings, tf.float32), pred))))\n",
    "\n",
    "\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(tf.cast(ratings, tf.float32), \n",
    "#                                                   tf.gather_nd(dot, indices)))))\n",
    "\n",
    "\n",
    "reg_pu = tf.contrib.layers.l2_regularizer(reg)(pu)\n",
    "reg_qi = tf.contrib.layers.l2_regularizer(reg)(qi)\n",
    "reg_bu = tf.contrib.layers.l2_regularizer(reg)(bu)\n",
    "reg_bi = tf.contrib.layers.l2_regularizer(reg)(bi)\n",
    "reg_yj = tf.contrib.layers.l2_regularizer(reg)(yj)\n",
    "total_loss = tf.add_n([loss, reg_pu, reg_qi, reg_bu, reg_bi, reg_yj])\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "training_op = optimizer.minimize(total_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "print(\"before time: \", time.time() - start_time)\n",
    "\n",
    "batch_size = 10000\n",
    "for epoch in range(n_epochs):\n",
    "    n_batches = len(train_ratings) // batch_size\n",
    "    t0 = time.time()\n",
    "    for i in range(n_batches):\n",
    "        end = min((i+1) * batch_size, len(train_ratings))\n",
    "        train_batch_ratings = train_ratings[i * batch_size: end]\n",
    "        train_batch_indices = train_indices[i * batch_size: end]\n",
    "        sess.run(training_op, feed_dict={ratings: train_batch_ratings, \n",
    "                                         indices: train_batch_indices, \n",
    "                                         global_mean: np.mean(train_ratings)})\n",
    "        \n",
    "#        print(\"nu: \", pu.eval()[:3, :3])\n",
    "\n",
    "    print(\"Epoch: \", epoch + 1, \"training time: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % display_step == 0:\n",
    "        t1 = time.time()\n",
    "        train_loss = loss.eval(feed_dict={ratings: train_ratings, \n",
    "                                          indices: train_indices,\n",
    "                                          global_mean: np.mean(train_ratings)})\n",
    "        test_loss = loss.eval(feed_dict={ratings: test_ratings, \n",
    "                                         indices: test_indices, \n",
    "                                         global_mean: np.mean(train_ratings)})\n",
    "        print(\"evaluate time: \", time.time() - t1)\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttrain loss: \", train_loss, \"\\ttest loss: \", test_loss)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <center> Self Implementation </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ratings(dataset):\n",
    "    for user, r in data.items():\n",
    "        for item, rating in r.items():\n",
    "            yield user, item, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse_slow(data=\"train\"):\n",
    "    if data == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "    elif data == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        \n",
    "    score = 0\n",
    "    for u, i, rating in zip(user_indices, item_indices, ratings):\n",
    "        pred = np.dot(pu[u], qi[i])\n",
    "        score += np.power((rating - pred), 2)\n",
    "    return np.sqrt(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "    elif mode == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        \n",
    "    pred = np.dot(pu, qi.T)[user_indices, item_indices]\n",
    "    score = np.sqrt(np.mean(np.power(pred - ratings, 2)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse_bias(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "    elif mode == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        \n",
    "    global_mean = np.mean(train_ratings)    \n",
    "    pred = global_mean + bu[user_indices] + bi[item_indices] + \\\n",
    "           np.dot(pu, qi.T)[user_indices, item_indices]\n",
    "    score = np.sqrt(np.mean(np.power(pred - ratings, 2)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \ttraining RMSE:  2.9362434999652156 \ttest RMSE:  4.098058099608248\n",
      "Epoch:  6 \ttraining RMSE:  0.9600320510483358 \ttest RMSE:  2.1875638551408567\n",
      "Epoch:  11 \ttraining RMSE:  0.803399926953229 \ttest RMSE:  1.7524336067426713\n",
      "Epoch:  16 \ttraining RMSE:  0.7746065010862653 \ttest RMSE:  1.5165011842067824\n",
      "Epoch:  21 \ttraining RMSE:  0.7692515864275132 \ttest RMSE:  1.3652587199037947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9f7aff607b18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'np.random.seed(42)\\nn_factors = 100\\nn_users = len(user2id)\\nn_items = len(item2id)\\nglobal_mean = np.mean(train_ratings)\\n\\nn_epochs = 200\\nlr = 0.002\\nreg = 0.1\\n\\nbu = np.zeros((n_users))\\nbi = np.zeros((n_items))\\npu = np.random.normal(size=(n_users, n_factors))\\nqi = np.random.normal(size=(n_items, n_factors))\\nfor epoch in range(n_epochs):\\n    for u, i, r in ratings(data):\\n        dot = np.dot(qi[i], pu[u])\\n        err = r - (global_mean + bu[u] + bi[i] + dot)\\n        bu[u] += lr * (err - reg * bu[u])\\n        bi[i] += lr * (err - reg * bi[i])\\n        qi[i] += lr * (err * pu[u] - reg * qi[i])\\n        pu[u] += lr * (err * qi[i] - reg * pu[u])\\n    \\n    if epoch % 5 == 0:\\n        print(\"Epoch: \", epoch + 1, \"\\\\ttraining RMSE: \", compute_rmse_bias(\"train\"), \\n              \"\\\\ttest RMSE: \", compute_rmse_bias(\"test\"))'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "global_mean = np.mean(train_ratings)\n",
    "\n",
    "n_epochs = 200\n",
    "lr = 0.002\n",
    "reg = 0.1   ############################ reg\n",
    "\n",
    "bu = np.zeros((n_users))\n",
    "bi = np.zeros((n_items))\n",
    "pu = np.random.normal(size=(n_users, n_factors))\n",
    "qi = np.random.normal(size=(n_items, n_factors))\n",
    "for epoch in range(n_epochs):\n",
    "    for u, i, r in ratings(data):\n",
    "        dot = np.dot(qi[i], pu[u])\n",
    "        err = r - (global_mean + bu[u] + bi[i] + dot)\n",
    "        bu[u] += lr * (err - reg * bu[u])\n",
    "        bi[i] += lr * (err - reg * bi[i])\n",
    "        qi[i] += lr * (err * pu[u] - reg * qi[i])\n",
    "        pu[u] += lr * (err * qi[i] - reg * pu[u])\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttraining RMSE: \", compute_rmse_bias(\"train\"), \n",
    "              \"\\ttest RMSE: \", compute_rmse_bias(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SVD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse_implicit(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "    elif mode == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        \n",
    "    global_mean = np.mean(train_ratings)\n",
    "#    nu_sqrt = np.sqrt(len(data[u]))\n",
    "    nu = np.array([np.sum(yj[list(data[u].keys())], axis=0) / np.sqrt(len(data[u])) for u in data.keys()])\n",
    "    pred = global_mean + bu[user_indices] + bi[item_indices] + \\\n",
    "           np.dot(pu + nu, qi.T)[user_indices, item_indices]\n",
    "    score = np.sqrt(np.mean(np.power(pred - ratings, 2)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \ttime:  1610.9779999256134\n",
      "Epoch:  1 \ttraining RMSE:  3.3552425198797797 \ttest RMSE:  4.407144461077849\n",
      "Epoch:  2 \ttime:  1644.4771127700806\n",
      "Epoch:  2 \ttraining RMSE:  2.0716387279736987 \ttest RMSE:  3.329736071044476\n",
      "Epoch:  3 \ttime:  1653.775175333023\n",
      "Epoch:  3 \ttraining RMSE:  1.5287149341135262 \ttest RMSE:  2.8417261609841877\n",
      "Epoch:  4 \ttime:  1641.5009999275208\n",
      "Epoch:  4 \ttraining RMSE:  1.244712407614461 \ttest RMSE:  2.549848920069196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-63418544912e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'np.random.seed(42)\\nn_factors = 100\\nn_users = len(user2id)\\nn_items = len(item2id)\\nglobal_mean = np.mean(train_ratings)\\n\\nn_epochs = 200\\nlr = 0.002\\nreg = 0.1\\n\\nbu = np.zeros((n_users))\\nbi = np.zeros((n_items))\\npu = np.random.normal(size=(n_users, n_factors))\\nqi = np.random.normal(size=(n_items, n_factors))\\nyj = np.random.normal(size=(n_items, n_factors))\\n\\nfor epoch in range(n_epochs):\\n    t0 = time.time()\\n    for u, i, r in ratings(data):\\n        t1 = time.time()\\n        nu_sqrt = np.sqrt(len(data[u]))\\n        nu = np.sum(yj[list(data[u].keys())], axis=0) / nu_sqrt\\n        dot = np.dot(qi[i], pu[u] + nu)\\n        err = r - (global_mean + bu[u] + bi[i] + dot)\\n        bu[u] += lr * (err - reg * bu[u])\\n        bi[i] += lr * (err - reg * bi[i])\\n        qi[i] += lr * (err * (pu[u] + nu) - reg * qi[i])\\n        pu[u] += lr * (err * qi[i] - reg * pu[u])\\n\\n        for j in data[u]:\\n            yj[j] += lr * (err * qi[i] / nu_sqrt - reg * yj[j])\\n    print(\"Epoch: \", epoch + 1, \"\\\\ttime: \", time.time() - t0)\\n    \\n    if epoch % 1 == 0:\\n        print(\"Epoch: \", epoch + 1, \"\\\\ttraining RMSE: \", compute_rmse_implicit(\"train\"), \\n              \"\\\\ttest RMSE: \", compute_rmse_implicit(\"test\"))'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "global_mean = np.mean(train_ratings)\n",
    "\n",
    "n_epochs = 200\n",
    "lr = 0.002\n",
    "reg = 0.1\n",
    "\n",
    "bu = np.zeros((n_users))\n",
    "bi = np.zeros((n_items))\n",
    "pu = np.random.normal(size=(n_users, n_factors))\n",
    "qi = np.random.normal(size=(n_items, n_factors))\n",
    "yj = np.random.normal(size=(n_items, n_factors))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t0 = time.time()\n",
    "    for u, i, r in ratings(data):\n",
    "        t1 = time.time()\n",
    "        nu_sqrt = np.sqrt(len(data[u]))\n",
    "        nu = np.sum(yj[list(data[u].keys())], axis=0) / nu_sqrt\n",
    "        dot = np.dot(qi[i], pu[u] + nu)\n",
    "        err = r - (global_mean + bu[u] + bi[i] + dot)\n",
    "        bu[u] += lr * (err - reg * bu[u])\n",
    "        bi[i] += lr * (err - reg * bi[i])\n",
    "        qi[i] += lr * (err * (pu[u] + nu) - reg * qi[i])\n",
    "        pu[u] += lr * (err * qi[i] - reg * pu[u])\n",
    "\n",
    "        for j in data[u]:\n",
    "            yj[j] += lr * (err * qi[i] / nu_sqrt - reg * yj[j])\n",
    "    print(\"Epoch: \", epoch + 1, \"\\ttime: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttraining RMSE: \", compute_rmse_implicit(\"train\"), \n",
    "              \"\\ttest RMSE: \", compute_rmse_implicit(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \ttime:  430.5702919960022\n",
      "Epoch:  1 \ttraining RMSE:  2.986250850310729 \ttest RMSE:  4.119221831354795\n",
      "evaluate time:  1.342958927154541\n",
      "Epoch:  2 \ttime:  430.4068841934204\n",
      "Epoch:  2 \ttraining RMSE:  1.8967664819373093 \ttest RMSE:  3.2052684515603778\n",
      "evaluate time:  1.67173171043396\n",
      "Epoch:  3 \ttime:  410.6034822463989\n",
      "Epoch:  3 \ttraining RMSE:  1.4367741172250574 \ttest RMSE:  2.7772792417419585\n",
      "evaluate time:  1.5271949768066406\n",
      "Epoch:  4 \ttime:  401.0427813529968\n",
      "Epoch:  4 \ttraining RMSE:  1.1929674473569962 \ttest RMSE:  2.512029929104096\n",
      "evaluate time:  1.4937841892242432\n",
      "Epoch:  5 \ttime:  403.96313071250916\n",
      "Epoch:  5 \ttraining RMSE:  1.0497991926666308 \ttest RMSE:  2.3239321630364063\n",
      "evaluate time:  1.539628505706787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "global_mean = np.mean(train_ratings)\n",
    "\n",
    "n_epochs = 200\n",
    "lr = 0.002\n",
    "reg = 0.1\n",
    "\n",
    "bu = np.zeros((n_users))\n",
    "bi = np.zeros((n_items))\n",
    "pu = np.random.normal(size=(n_users, n_factors))\n",
    "qi = np.random.normal(size=(n_items, n_factors))\n",
    "yj = np.random.normal(size=(n_items, n_factors))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t0 = time.time()\n",
    "    for u, i, r in ratings(data):\n",
    "        u_items = list(data[u].keys())\n",
    "        nu_sqrt = np.sqrt(len(u_items))\n",
    "        nu = np.sum(yj[u_items], axis=0) / nu_sqrt\n",
    "        dot = np.dot(qi[i], pu[u] + nu)\n",
    "        err = r - (global_mean + bu[u] + bi[i] + dot)\n",
    "        bu[u] += lr * (err - reg * bu[u])\n",
    "        bi[i] += lr * (err - reg * bi[i])\n",
    "        qi[i] += lr * (err * (pu[u] + nu) - reg * qi[i])\n",
    "        pu[u] += lr * (err * qi[i] - reg * pu[u])\n",
    "        yj[u_items] += lr * (err * qi[u_items] / nu_sqrt - reg * yj[u_items])\n",
    "    print(\"Epoch: \", epoch + 1, \"\\ttime: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        t1 = time.time()\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttraining RMSE: \", compute_rmse_implicit(\"train\"), \n",
    "              \"\\ttest RMSE: \", compute_rmse_implicit(\"test\"))\n",
    "        print(\"evaluate time: \", time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
