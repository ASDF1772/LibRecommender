{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from pdb import set_trace\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "train_set = open(\"ml-1m/ratings.dat\").readlines()\n",
    "train_set = np.random.permutation(train_set)[200000:400000]\n",
    "train_threshold = int(0.8 * len(train_set))\n",
    "# train_indices = []\n",
    "# test_indices = []\n",
    "train_user_indices = []\n",
    "train_item_indices = []\n",
    "test_user_indices = []\n",
    "test_item_indices = []\n",
    "train_ratings = []\n",
    "test_ratings = []\n",
    "\n",
    "data = defaultdict(dict)\n",
    "user2id = {}\n",
    "item2id = {}\n",
    "index_user = 0\n",
    "index_item = 0\n",
    "\n",
    "for i, line in enumerate(train_set):\n",
    "    user = line.split(\"::\")[0]\n",
    "    item = line.split(\"::\")[1]\n",
    "    rating = line.split(\"::\")[2]\n",
    "    \n",
    "    try:\n",
    "        user_id = user2id[user]\n",
    "    except KeyError:\n",
    "        user_id = index_user\n",
    "        user2id[user] = index_user\n",
    "        index_user += 1\n",
    "    try:\n",
    "        item_id = item2id[item]\n",
    "    except KeyError:\n",
    "        item_id = index_item\n",
    "        item2id[item] = index_item\n",
    "        index_item += 1\n",
    "        \n",
    "    if i < train_threshold:\n",
    "    #    train_indices.append((user_id, item_id))\n",
    "        train_user_indices.append(user_id)\n",
    "        train_item_indices.append(item_id)\n",
    "        train_ratings.append(int(rating))\n",
    "        data[user_id].update(dict(zip([item_id], [int(rating)])))\n",
    "    else:\n",
    "    #    test_indices.append((user_id, item_id))\n",
    "        test_user_indices.append(user_id)\n",
    "        test_item_indices.append(item_id)\n",
    "        test_ratings.append(int(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 40000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ratings), len(test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  160000 40000\n",
      "after:  160000 39967\n"
     ]
    }
   ],
   "source": [
    "print(\"before: \", len(train_ratings), len(test_ratings))\n",
    "\n",
    "for u, i, r in zip(test_user_indices, test_item_indices, test_ratings):\n",
    "    if u not in data:\n",
    "        test_user_indices.remove(u)\n",
    "        test_item_indices.remove(i)\n",
    "        test_ratings.remove(r)\n",
    "\n",
    "for u, i, r in zip(test_user_indices, test_item_indices, test_ratings):\n",
    "    if u not in data:\n",
    "        test_user_indices.remove(u)\n",
    "        test_item_indices.remove(i)\n",
    "        test_ratings.remove(r)\n",
    "\n",
    "for u, i, r in zip(test_user_indices, test_item_indices, test_ratings):\n",
    "    if u not in data:\n",
    "        test_user_indices.remove(u)\n",
    "        test_item_indices.remove(i)\n",
    "        test_ratings.remove(r)\n",
    "\n",
    "for u in test_user_indices:\n",
    "    if u not in data:\n",
    "        print(\"left\", u)\n",
    "\n",
    "print(\"after: \", len(train_ratings), len(test_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratings(dataset):\n",
    "    for user, r in data.items():\n",
    "        for item, rating in r.items():\n",
    "            yield user, item, rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate bbu & bbi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "    elif mode == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        \n",
    "    global_mean = np.mean(train_ratings)\n",
    "    pred = global_mean + bbu[user_indices] + bbi[item_indices] \n",
    "    score = np.sqrt(np.mean(np.power(pred - ratings, 2)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 training RMSE:  1.1038086908163407 test RMSE:  1.0985982825866456\n",
      "\n",
      " Converged !\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "global_mean = np.mean(train_ratings)\n",
    "n_epochs = 200\n",
    "lr = 0.002\n",
    "reg = 0.1\n",
    "bbu = np.zeros((n_users,))\n",
    "bbi = np.zeros((n_items,))\n",
    "for epoch in range(n_epochs):\n",
    "    for u, i, r in ratings(data):\n",
    "        before = bbu.copy()\n",
    "        err = r - (global_mean + bbu[u] + bbi[i])\n",
    "        bbu[u] += lr * (err - reg * bbu[u])\n",
    "        bbi[i] += lr * (err - reg * bbi[i])\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch: \", epoch + 1, \"training RMSE: \", compute_rmse(\"train\"), \"test RMSE: \", compute_rmse(\"test\"))\n",
    "        if np.allclose(before, bbu, atol=1e-3):\n",
    "            print(\"\\n Converged !\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### neighborhood model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before time 9.674000024795532\n",
      "Epoch:  1 training time:  204.36899995803833\n",
      "Epoch:  1 \ttrain loss:  1.0718961 \ttest loss:  1.0989753\n",
      "evaluate time:  2.684000015258789\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0a6dde598725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         sess.run(training_op, feed_dict={user_indices: train_user_indices, \n\u001b[1;32m     84\u001b[0m                                          \u001b[0mitem_indices\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_item_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                                          ratings: train_ratings})\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"training time: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "global_mean = np.mean(train_ratings)\n",
    "base_user = bbu\n",
    "base_item = bbi\n",
    "\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "reg = 1e-4\n",
    "display_step = 1\n",
    "\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "w = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "c = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "\n",
    "user_indices = tf.placeholder(tf.int32, shape=[None])\n",
    "item_indices = tf.placeholder(tf.int32, shape=[None])\n",
    "ratings = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "\n",
    "expanded_items = np.zeros((n_users, n_items), dtype=np.int32)\n",
    "for u in data:\n",
    "    u_items = list(data[u].keys())\n",
    "    expanded_items[u] = np.array(u_items + [-1] * (n_items - len(u_items)))\n",
    "    \n",
    "expanded_ratings = np.zeros((n_users, n_items), dtype=np.int32)\n",
    "for u in data:\n",
    "    u_ratings = list(data[u].values())\n",
    "    expanded_ratings[u] = np.array(u_ratings + [-1] * (n_items - len(u_ratings)))\n",
    "\n",
    "def predict_func(elem):\n",
    "    u = elem[0]\n",
    "    i = elem[1]\n",
    "#    r = elem[2]\n",
    "    mask = tf.constant(-1, dtype=tf.int32)\n",
    "    u_expanded_items = tf.nn.embedding_lookup(expanded_items, u)\n",
    "    u_items = tf.boolean_mask(u_expanded_items, tf.not_equal(u_expanded_items, mask))\n",
    "    \n",
    "    u_expanded_ratings = tf.nn.embedding_lookup(expanded_ratings, u)\n",
    "    u_ratings = tf.boolean_mask(u_expanded_ratings, tf.not_equal(u_expanded_ratings, mask))\n",
    "    \n",
    "    base_neighbor = global_mean + tf.gather(base_user, u) + tf.gather(base_item, u_items)\n",
    "    w_ij = tf.gather(tf.gather(w, i), u_items)\n",
    "    c_ij = tf.gather(tf.gather(c, i), u_items)\n",
    "    \n",
    "#    ru = tf.reduce_sum((tf.cast(u_ratings, tf.float32) - tf.cast(base_neighbor, tf.float32)) * \n",
    "#                       w_ij) / tf.sqrt(tf.cast(tf.size(u_items), tf.float32))\n",
    "    u_ratings = tf.cast(u_ratings, tf.float32)\n",
    "    base_neighbor = tf.cast(base_neighbor, tf.float32)\n",
    "    ru = tf.reduce_sum(tf.multiply(tf.subtract(u_ratings, base_neighbor), w_ij))\n",
    "    ru = tf.divide(ru, tf.sqrt(tf.cast(tf.size(u_items), tf.float32)))\n",
    "    nu = tf.reduce_sum(c_ij) / tf.sqrt(tf.cast(tf.size(u_items), tf.float32))\n",
    "    pred = global_mean + tf.gather(bu, u) + tf.gather(bi, i) + ru + nu\n",
    "    return pred\n",
    "    \n",
    "# pred = tf.map_fn(predict_func, (np.array(train_user_indices), np.array(train_item_indices)), dtype=tf.float32)\n",
    "pred = tf.map_fn(predict_func, (user_indices, item_indices), dtype=tf.float32)\n",
    "pred = tf.convert_to_tensor(pred)\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(tf.cast(ratings, tf.float32), pred))))\n",
    "    \n",
    "\n",
    "reg_bu = tf.contrib.layers.l2_regularizer(reg)(bu)\n",
    "reg_bi = tf.contrib.layers.l2_regularizer(reg)(bi)\n",
    "reg_w = tf.contrib.layers.l2_regularizer(reg)(w)\n",
    "reg_c = tf.contrib.layers.l2_regularizer(reg)(c)\n",
    "total_loss = tf.add_n([loss, reg_bu, reg_bi, reg_w, reg_c])\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "training_op = optimizer.minimize(total_loss)\n",
    "init = tf.global_variables_initializer()\n",
    "print(\"before time\", time.time() - start)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        t0 = time.time()\n",
    "        sess.run(training_op, feed_dict={user_indices: train_user_indices, \n",
    "                                         item_indices: train_item_indices, \n",
    "                                         ratings: train_ratings})\n",
    "        print(\"Epoch: \", epoch+1, \"training time: \", time.time() - t0)\n",
    "        t1 = time.time()\n",
    "        if epoch % display_step == 0:\n",
    "            train_loss = loss.eval(feed_dict={user_indices: train_user_indices, \n",
    "                                              item_indices: train_item_indices, \n",
    "                                              ratings: train_ratings})\n",
    "            test_loss = loss.eval(feed_dict={user_indices: test_user_indices, \n",
    "                                             item_indices: test_item_indices, \n",
    "                                             ratings: test_ratings})\n",
    "            print(\"Epoch: \", epoch + 1, \"\\ttrain loss: \", train_loss, \"\\ttest loss: \", test_loss)\n",
    "            print(\"evaluate time: \", time.time() - t1)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Execution - neighborhood model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# bbu, bbi = pickle.load(open(\"bbu_bbi.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "np.random.seed(42)\n",
    "tf.reset_default_graph()\n",
    "n_factors = 100\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "global_mean = np.mean(train_ratings)\n",
    "base_user = bbu\n",
    "base_item = bbi\n",
    "\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "reg = 1e-4\n",
    "display_step = 1\n",
    "\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "w = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "c = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "\n",
    "\n",
    "def compute_rmse_neighbor(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "    elif mode == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        \n",
    "    score = 0\n",
    "    pred_whole = []\n",
    "    for u, i in zip(user_indices, item_indices):\n",
    "        u_items = list(data[u].keys())\n",
    "        u_ratings = np.array(list(data[u].values()))\n",
    "        base_neighbor = global_mean + bbu[u] + bbi[u_items]\n",
    "#        ru = np.sum((u_ratings - base_neighbor) * w[i][u_items]) / np.sqrt(len(u_items))\n",
    "#        nu = np.sum(c[i][u_items]) / np.sqrt(len(u_items))\n",
    "        if len(data[u]) == 0:\n",
    "            ru = 0\n",
    "            nu = 0\n",
    "        else:\n",
    "            ru = tf.reduce_sum((u_ratings - base_neighbor) * tf.gather(tf.gather(w, i), u_items)) / \\\n",
    "                tf.sqrt(tf.cast(tf.size(u_items), tf.float32))\n",
    "            nu = tf.reduce_sum(tf.gather(tf.gather(c, i), u_items)) / tf.sqrt(tf.cast(tf.size(u_items), tf.float32))\n",
    "        pred = global_mean + bu[u] + bi[i] + ru + nu\n",
    "        pred_whole.append(pred)\n",
    "    pred_whole = tf.convert_to_tensor(pred_whole)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.pow(pred_whole - np.array(ratings), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \ttime:  20.796000003814697\n",
      "Epoch:  1 \ttraining RMSE:  0.91137004 \ttest RMSE:  1.1005447\n",
      "evaluate time:  6.771999835968018\n",
      "Epoch:  2 \ttime:  19.57800006866455\n",
      "Epoch:  2 \ttraining RMSE:  0.90764153 \ttest RMSE:  1.1006138\n",
      "evaluate time:  6.5920000076293945\n",
      "Epoch:  3 \ttime:  23.756999969482422\n",
      "Epoch:  3 \ttraining RMSE:  0.9039209 \ttest RMSE:  1.1006826\n",
      "evaluate time:  6.863000154495239\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a62e6d204afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_rmse_neighbor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       output_gradients)\n\u001b[0m",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_GatherGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1457\u001b[0m   \u001b[0mparams_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m   \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m   \u001b[0mvalues_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m   \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m   \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10542\u001b[0m         \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m  10543\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"new_axis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shrink_axis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m> 10544\u001b[0;31m         shrink_axis_mask)\n\u001b[0m\u001b[1;32m  10545\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m  10546\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "# optimizer = tf.train.AdamOptimizer(lr)\n",
    "for epoch in range(n_epochs):\n",
    "    t0 = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        variables = [bu, bi, w, c]\n",
    "        loss = compute_rmse_neighbor()\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        \n",
    "    print(\"Epoch: \", epoch + 1, \"\\ttime: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        t1 = time.time()\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttraining RMSE: \", compute_rmse_neighbor(\"train\").numpy(), \n",
    "              \"\\ttest RMSE: \", compute_rmse_neighbor(\"test\").numpy())\n",
    "        print(\"evaluate time: \", time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Execution - neighborhood model K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "bbu, bbi = pickle.load(open(\"bbu_bbi.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symetric\n",
      "CPU times: user 40.1 s, sys: 228 ms, total: 40.3 s\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(dicts, x1, x2):  # only common ratings\n",
    "    prod = denom1 = denom2 = 0\n",
    "    for item in dicts[x1]:\n",
    "        if item in dicts[x2]:\n",
    "            prod += dicts[x1][item] * dicts[x2][item]\n",
    "            denom1 += dicts[x1][item] ** 2\n",
    "            denom2 += dicts[x2][item] ** 2\n",
    "    try:\n",
    "        return prod / sqrt(denom1 * denom2)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "def map_func(i):\n",
    "    return [cosine_similarity(data, i, other) for other in item_idsss]\n",
    "\n",
    "\n",
    "def get_sim(n, item_ids, parallel=False, symmetric=True, n_jobs=4):\n",
    "    if not parallel and not symmetric:\n",
    "        print(\"not symmetric\")\n",
    "        sim = np.array([[cosine_similarity(data, i, other) for other in item_ids] for i in item_ids])\n",
    "    if not parallel and symmetric:\n",
    "        print(\"symetric\")\n",
    "        sim = np.zeros((n, n))\n",
    "        for i in item_ids:   ################# item_ids\n",
    "            sim[i, i] = 1.0\n",
    "            for j in item_ids[i + 1:]:\n",
    "                sim[i, j] = cosine_similarity(data, i, j)\n",
    "                sim[j, i] = sim[i, j]\n",
    "    if parallel:\n",
    "        print(\"multiprocessing\")\n",
    "        with Pool(processes=n_jobs) as p:\n",
    "            sim = p.map(map_func, item_ids)\n",
    "            sim = np.array(sim)\n",
    "    return sim\n",
    "\n",
    "item_idsss = list(item2id.values())   # 排除 test data\n",
    "n_items = len(item2id)\n",
    "%time sim_matrix = get_sim(n_items, item_idsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_whole = {}\n",
    "for i in range(n_items):\n",
    "    sim_whole[i] = np.argsort(sim_matrix[i])[::-1]\n",
    "    \n",
    "k = 20\n",
    "intersect_user_item = {}\n",
    "for user, item, _ in ratings(data):\n",
    "    u_items = list(data[user].keys())\n",
    "    sim_items = sim_whole[item][:k]\n",
    "    intersect_items, index_u, _ = np.intersect1d(\n",
    "        u_items, sim_items, assume_unique=True, return_indices=True)\n",
    "    intersect_user_item[(user, item)] = (intersect_items, index_u)\n",
    "    \n",
    "intersect_user_item_test = {}\n",
    "for user, item in zip(test_user_indices, test_item_indices):\n",
    "    u_items = list(data[user].keys())\n",
    "    sim_items = sim_whole[item][:k]\n",
    "    intersect_items, index_u, _ = np.intersect1d(u_items, sim_items, assume_unique=True, return_indices=True)\n",
    "    intersect_user_item_test[(user, item)] = (intersect_items, index_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_neighbor_k(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "        intersect = intersect_user_item\n",
    "    elif mode == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        intersect = intersect_user_item_test\n",
    "        \n",
    "    pred_whole = []\n",
    "    for u, i in zip(user_indices, item_indices):\n",
    "        intersect_items, index_u = intersect[(u, i)]\n",
    "        \n",
    "        if tf.size(intersect_items).numpy() == 0:\n",
    "            pred = global_mean + bu[u] + bi[i]\n",
    "        else:\n",
    "            u_ratings = np.array(list(data[u].values()))[index_u]\n",
    "            base_neighbor = global_mean + bbu[u] + bbi[intersect_items]\n",
    "            user_sqrt = tf.sqrt(tf.cast(tf.size(intersect_items), tf.float32))\n",
    "            ru = tf.reduce_sum((u_ratings - base_neighbor) * tf.gather(tf.gather(w, i), intersect_items)) / user_sqrt\n",
    "            nu = tf.reduce_sum(tf.gather(tf.gather(c, i), intersect_items)) / user_sqrt\n",
    "            pred = global_mean + bu[u] + bi[i] + ru + nu\n",
    "        pred_whole.append(pred)\n",
    "\n",
    "    pred_whole = tf.convert_to_tensor(pred_whole)\n",
    "    score = tf.sqrt(tf.reduce_mean(tf.pow(pred_whole - np.array(ratings), 2)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \ttime:  80.52100014686584\n",
      "Epoch:  1 \ttraining RMSE:  1.1189075 \ttest RMSE:  1.1092225\n",
      "evaluate time:  34.08999991416931\n",
      "Epoch:  2 \ttime:  74.84100008010864\n",
      "Epoch:  2 \ttraining RMSE:  1.1189063 \ttest RMSE:  1.1092225\n",
      "evaluate time:  32.74799990653992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ee8168554aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_rmse_neighbor_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       output_gradients)\n\u001b[0m",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py\u001b[0m in \u001b[0;36m_GatherV2Grad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0mparams_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     \u001b[0mparams_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_int32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    248\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   8803\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8804\u001b[0;31m         _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[1;32m   8805\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ee8168554aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_rmse_neighbor_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\ttime: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m   \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m     \u001b[1;34m\"\"\"Exits the recording context, no further operations are traced.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_factors = 100\n",
    "k = 20  ###############\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "global_mean = np.mean(train_ratings)\n",
    "base_user = bbu\n",
    "base_item = bbi\n",
    "\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "reg = 1e-4\n",
    "display_step = 1\n",
    "\n",
    "bu = tf.Variable(tf.zeros([n_users]))\n",
    "bi = tf.Variable(tf.zeros([n_items]))\n",
    "w = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "c = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "# optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t0 = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        variables = [bu, bi, w, c]\n",
    "        loss = compute_rmse_neighbor_k()\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "    print(\"Epoch: \", epoch + 1, \"\\ttime: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        t1 = time.time()\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttraining RMSE: \", compute_rmse_neighbor_k(\"train\").numpy(), \n",
    "              \"\\ttest RMSE: \", compute_rmse_neighbor_k(\"test\").numpy())\n",
    "        print(\"evaluate time: \", time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Eager Execution - superSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "bbu, bbi = pickle.load(open(\"bbu_bbi.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    if len(data[i]) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_superSVD(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "        intersect = intersect_user_item\n",
    "    elif mode == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        intersect = intersect_user_item_test\n",
    "        \n",
    "    nui = []\n",
    "    for u in range(n_users):\n",
    "        u_items = np.array(list(data[u].keys()))\n",
    "        if tf.size(u_items).numpy() == 0:\n",
    "            nui.append(tf.zeros(n_factors, dtype=tf.float32))\n",
    "#            nui.append(np.zeros(100))\n",
    "        else:\n",
    "            nui.append(tf.reduce_sum(tf.gather(yj, u_items), axis=0) / tf.sqrt(tf.cast(tf.size(u_items), tf.float32)))\n",
    "            \n",
    "            \n",
    "#    nui = tf.convert_to_tensor(nui)\n",
    "    dot = tf.matmul(pu + nui, qi, transpose_b=True)\n",
    "    indices = np.array([user_indices, item_indices]).T\n",
    "    pred1 = global_mean + tf.gather(bu, user_indices) + tf.gather(bi, item_indices) + tf.gather_nd(dot, indices)\n",
    "   \n",
    "    \n",
    "    pred2 = []\n",
    "    for u, i in zip(user_indices, item_indices):\n",
    "        intersect_items, index_u = intersect[(u, i)]\n",
    "        \n",
    "        if tf.size(intersect_items).numpy() == 0:\n",
    "            pred2.append(0)\n",
    "        else:\n",
    "            u_ratings = np.array(list(data[u].values()))[index_u]\n",
    "            base_neighbor = global_mean + bbu[u] + bbi[intersect_items]\n",
    "            user_sqrt = tf.sqrt(tf.cast(tf.size(intersect_items), tf.float32))\n",
    "            ru = tf.cast(tf.reduce_sum((u_ratings - base_neighbor) * tf.gather(tf.gather(w, i), intersect_items)), tf.float32) / user_sqrt\n",
    "            nu = tf.cast(tf.reduce_sum(tf.gather(tf.gather(c, i), intersect_items)), tf.float32) / user_sqrt\n",
    "            pred2.append(ru + nu)\n",
    "            \n",
    "    pred_whole = pred1 + pred2\n",
    "    pred_whole = tf.convert_to_tensor(pred_whole)\n",
    "    score = tf.sqrt(tf.reduce_mean(tf.pow(pred_whole - ratings, 2))) + \\\n",
    "            reg * (tf.nn.l2_loss(bu) + tf.nn.l2_loss(bi) + tf.nn.l2_loss(pu) + \n",
    "                   tf.nn.l2_loss(qi) + tf.nn.l2_loss(yj) + tf.nn.l2_loss(w) + tf.nn.l2_loss(c))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \ttime:  2022.6800422668457\n",
      "Epoch:  1 \ttraining RMSE:  12.021564 \ttest RMSE:  12.021573\n",
      "evaluate time:  35.197007179260254\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "PyEval_EvalFrameEx returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[0;34m(gradients)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                 for x in indexed_slices_list], 0)\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexed_slices_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexedSlices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1123\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ConcatV2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         name, _ctx._post_execution_callbacks, values, axis)\n\u001b[0m\u001b[1;32m   1048\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f87b6800bee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_rmse_superSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       output_gradients)\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \"\"\"\n\u001b[0;32m--> 112\u001b[0;31m   \u001b[0mmock_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MockOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: PyEval_EvalFrameEx returned a result with an error set"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "k = 20   ############################\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "n_factors = 100\n",
    "global_mean = np.mean(train_ratings)\n",
    "n_epochs = 200\n",
    "lr = 0.001\n",
    "reg = 0.01\n",
    "\n",
    "bu = tf.Variable(tf.zeros([n_users,]))\n",
    "bi = tf.Variable(tf.zeros([n_items,]))\n",
    "pu = tf.Variable(tf.random_normal([n_users, n_factors], 0.0, 0.01))\n",
    "qi = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "yj = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "w = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "c = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t0 = time.time()\n",
    "    with tf.GradientTape() as tape:\n",
    "        variables = [bu, bi, pu, qi, yj, w, c]\n",
    "        loss = compute_rmse_superSVD()\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "    print(\"Epoch: \", epoch + 1, \"\\ttime: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        t1 = time.time()\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttraining RMSE: \", compute_rmse_superSVD(\"train\").numpy(), \n",
    "              \"\\ttest RMSE: \", compute_rmse_superSVD(\"test\").numpy())\n",
    "        print(\"evaluate time: \", time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_superSVD_batch(u, i, mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        user_indices = train_user_indices\n",
    "        item_indices = train_item_indices\n",
    "        ratings = train_ratings\n",
    "        intersect = intersect_user_item\n",
    "    elif mode == \"test\":\n",
    "        user_indices = test_user_indices\n",
    "        item_indices = test_item_indices\n",
    "        ratings = test_ratings\n",
    "        intersect = intersect_user_item_test\n",
    "\n",
    "    u_items = np.array(list(data[u].keys()))\n",
    "    nui = tf.reduce_sum(tf.gather(yj, u_items), axis=0) / tf.sqrt(tf.cast(tf.size(u_items), tf.float32))\n",
    "            \n",
    "            \n",
    "#    nui = tf.convert_to_tensor(nui)\n",
    "    dot = tf.reduce_sum(tf.multiply(tf.gather(pu, u) + nui, tf.gather(qi, i)))\n",
    "#    indices = np.array([user_indices, item_indices]).T\n",
    "    pred1 = global_mean + tf.gather(bu, u) + tf.gather(bi, i) + dot\n",
    "   \n",
    "\n",
    "    intersect_items, index_u = intersect[(u, i)]\n",
    "    u_ratings = np.array(list(data[u].values()))[index_u]\n",
    "    base_neighbor = global_mean + bbu[u] + bbi[intersect_items]\n",
    "    user_sqrt = tf.sqrt(tf.cast(tf.size(intersect_items), tf.float32))\n",
    "    ru = tf.cast(tf.reduce_sum((u_ratings - base_neighbor) * tf.gather(tf.gather(w, i), intersect_items)), tf.float32) / user_sqrt\n",
    "    nu = tf.cast(tf.reduce_sum(tf.gather(tf.gather(c, i), intersect_items)), tf.float32) / user_sqrt\n",
    "    pred2 = ru + nu\n",
    "            \n",
    "    pred_whole = pred1 + pred2\n",
    "    pred_whole = tf.convert_to_tensor(pred_whole)\n",
    "    score = tf.sqrt(tf.reduce_mean(tf.pow(pred_whole - ratings, 2))) + \\\n",
    "            reg * (tf.nn.l2_loss(bu) + tf.nn.l2_loss(bi) + tf.nn.l2_loss(pu) + \n",
    "                   tf.nn.l2_loss(qi) + tf.nn.l2_loss(yj) + tf.nn.l2_loss(w) + tf.nn.l2_loss(c))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \ttime:  570.9743838310242\n",
      "Epoch:  1 \ttraining RMSE:  nan \ttest RMSE:  nan\n",
      "evaluate time:  4.376272678375244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0d60482dcee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_rmse_superSVD_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\ttime: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    608\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m           \u001b[0mupdate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[0;34m(self, optimizer, g)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \"Cannot use a constraint function on a sparse variable.\")\n\u001b[1;32m    165\u001b[0m       return optimizer._resource_apply_sparse_duplicate_indices(\n\u001b[0;32m--> 166\u001b[0;31m           g.values, self._v, g.indices)\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/gradient_descent.py\u001b[0m in \u001b[0;36m_resource_apply_sparse_duplicate_indices\u001b[0;34m(self, grad, handle, indices)\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_resource_apply_sparse_duplicate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     return resource_variable_ops.resource_scatter_add(\n\u001b[0;32m---> 70\u001b[0;31m         handle.handle, indices, -grad * self._learning_rate)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_apply_sparse_duplicate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_scatter_add\u001b[0;34m(resource, indices, updates, name)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;34m\"ResourceScatterAdd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         indices, updates)\n\u001b[0m\u001b[1;32m    687\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "k = 20   ############################\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "n_factors = 100\n",
    "global_mean = np.mean(train_ratings)\n",
    "n_epochs = 200\n",
    "lr = 0.001\n",
    "reg = 1.0\n",
    "\n",
    "bu = tf.Variable(tf.zeros([n_users,]))\n",
    "bi = tf.Variable(tf.zeros([n_items,]))\n",
    "pu = tf.Variable(tf.random_normal([n_users, n_factors], 0.0, 0.01))\n",
    "qi = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "yj = tf.Variable(tf.random_normal([n_items, n_factors], 0.0, 0.01))\n",
    "w = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "c = tf.Variable(tf.random_normal([n_items, n_items], 0.0, 0.01))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "# optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t0 = time.time()\n",
    "    for u, i in zip(train_user_indices, train_item_indices):\n",
    "        with tf.GradientTape() as tape:\n",
    "            variables = [bu, bi, pu, qi, yj, w, c]\n",
    "            loss = compute_rmse_superSVD_batch(u, i)\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "    print(\"Epoch: \", epoch + 1, \"\\ttime: \", time.time() - t0)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        t1 = time.time()\n",
    "        print(\"Epoch: \", epoch + 1, \"\\ttraining RMSE: \", compute_rmse_superSVD(\"train\").numpy(), \n",
    "              \"\\ttest RMSE: \", compute_rmse_superSVD(\"test\").numpy())\n",
    "        print(\"evaluate time: \", time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
